{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting outcome of chess games based on my performance this year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:57:38.071106Z",
     "start_time": "2018-11-23T00:57:38.047868Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from time import time\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from dummies_bins_test_train_cv import get_Xy_train_test\n",
    "from different_xy import diff_bins, xy_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If new data is added, uncomment and run the next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T19:09:49.795074Z",
     "start_time": "2018-11-22T19:09:41.461564Z"
    }
   },
   "outputs": [],
   "source": [
    "from clean_chess_game_log import main_cleanup\n",
    "_,_,_ = main_cleanup('../data/dest.pgn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:57:48.211003Z",
     "start_time": "2018-11-23T00:57:48.185150Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/use_for_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:58:04.594002Z",
     "start_time": "2018-11-23T00:58:04.560838Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['result'] != 0.5].copy()\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:58:15.719872Z",
     "start_time": "2018-11-23T00:58:15.682303Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_df_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cdb4228aab65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_df_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_df_y' is not defined"
     ]
    }
   ],
   "source": [
    "df, y = clean_df_y(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the classifier:\n",
    "__The _Fun_ Part!!__\n",
    "\n",
    "### Ran the classifier with new data every time I played on Chess.com.\n",
    "\n",
    "Which is how I ended up with the following steps:\n",
    "\n",
    "- __Step 1__\n",
    "    - Add the sequential layer\n",
    "- __Step 2__\n",
    "    - Add the __Dense__ layer with following parameters:\n",
    "        - Units = 128\n",
    "        - Initializer = Uniform\n",
    "        - Activation = Softmax (Since the outcome is either 1 or 0, Softmax works wonders)\n",
    "        - Input Dim = the second part of the X Shape (# of columns)\n",
    "- __Step 3__\n",
    "    - Add the second __Dense__ layer with following parameters:\n",
    "        - Units = 256 (Double it up!!)\n",
    "        - Initializer = Normal (Throw a curve ball!)\n",
    "        - Activation = ReLu (Prevents Vanashing Gradiant problem)\n",
    "- __Step 4__\n",
    "    - Add the third __Dense__ layer with following parameters:\n",
    "        - Units = 64 (Make it easier for the compiling)\n",
    "        - Initializer = Uniform\n",
    "        - Activation = Softmax\n",
    "- __Step 5__\n",
    "    - Add the final __Dense__ layer with following parameters:\n",
    "        - Units = 1 (Just need 1 column for output)\n",
    "        - Initializer = Normal (#MuscleConfusion. I kid!)\n",
    "        - Activation = Sigmoid\n",
    "- __Step 6__\n",
    "    - Only think left is to __COMPILE__ it. From all the parameters I've tested. These work the best:\n",
    "        - Optimizer = Nadam (Nesterov Adam optimizer - Nadam is Adam RMSprop with Nesterov momentum)\n",
    "        - loss = Mean Absolute Error (Didn't get great results with binary_crossentropy, even though it was my first choice. Mean Squared Error, made error very small, since the loss is between 0 and 1.)\n",
    "        - metrics = Binary Accuracy (Only have 0 or 1 to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:59:08.595541Z",
     "start_time": "2018-11-23T00:59:08.567114Z"
    }
   },
   "outputs": [],
   "source": [
    "def _classifier():\n",
    "\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    classifier.add(Dense(units=64, activation='softmax', input_dim=X.shape[1]))\n",
    "    \n",
    "    classifier.add(Dense(units=128, activation='relu'))\n",
    "    \n",
    "    classifier.add(Dense(units=32, activation='softmax'))\n",
    "    \n",
    "    classifier.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    classifier.compile(\n",
    "        optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data to run predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test sets, along with clean the data\n",
    "### Also, see the shape to know the data is in right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:46:34.100922Z",
     "start_time": "2018-11-23T00:46:34.080666Z"
    }
   },
   "outputs": [],
   "source": [
    "all_cols_try = [\n",
    "    ['diff_bin'], ['color'], ['game_time'], ['time_bin'], ['weekday'],\n",
    "    ['diff_bin', 'color'], ['diff_bin', 'time_bin'], ['diff_bin', 'game_time'],\n",
    "    ['diff_bin', 'weekday'], ['diff_bin', 'color', 'time_bin'],\n",
    "    ['diff_bin', 'color', 'game_time'], ['diff_bin', 'color', 'weekday'],\n",
    "    ['diff_bin', 'time_bin', 'game_time'], ['diff_bin', 'time_bin', 'weekday'],\n",
    "    ['diff_bin', 'game_time', 'weekday'],\n",
    "    ['diff_bin', 'color', 'time_bin', 'game_time'],\n",
    "    ['diff_bin', 'color', 'time_bin', 'weekday'],\n",
    "    ['diff_bin', 'color', 'game_time', 'weekday'],\n",
    "    ['diff_bin', 'time_bin', 'game_time', 'weekday'],\n",
    "    ['diff_bin', 'color', 'game_time', 'time_bin', 'weekday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T00:46:34.914980Z",
     "start_time": "2018-11-23T00:46:34.890266Z"
    }
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T04:54:29.512775Z",
     "start_time": "2018-11-22T04:52:15.522177Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y Shape: (2117,)\n",
      "X Shape: (2117, 1)\n",
      "X_train Shape: (2011, 1)\n",
      "X_test Shape: (106, 1)\n",
      "y_train Shape: (2011,)\n",
      "y_test Shape: (106,)\n",
      "Epoch 1/96\n",
      "2011/2011 [==============================] - 1s 447us/step - loss: 0.4838 - acc: 0.6101\n",
      "Epoch 2/96\n",
      "2011/2011 [==============================] - 0s 185us/step - loss: 0.4212 - acc: 0.6937\n",
      "Epoch 3/96\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.3932 - acc: 0.6902\n",
      "Epoch 4/96\n",
      "2011/2011 [==============================] - 0s 173us/step - loss: 0.3757 - acc: 0.6917\n",
      "Epoch 5/96\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.3646 - acc: 0.6912\n",
      "Epoch 6/96\n",
      "2011/2011 [==============================] - 0s 190us/step - loss: 0.3548 - acc: 0.6912\n",
      "Epoch 7/96\n",
      "2011/2011 [==============================] - 0s 173us/step - loss: 0.3453 - acc: 0.6972\n",
      "Epoch 8/96\n",
      "2011/2011 [==============================] - 0s 175us/step - loss: 0.3384 - acc: 0.7001\n",
      "Epoch 9/96\n",
      "2011/2011 [==============================] - 0s 181us/step - loss: 0.3328 - acc: 0.7001\n",
      "Epoch 10/96\n",
      "2011/2011 [==============================] - 0s 207us/step - loss: 0.3283 - acc: 0.7001\n",
      "Epoch 11/96\n",
      "2011/2011 [==============================] - 0s 189us/step - loss: 0.3246 - acc: 0.7001\n",
      "Epoch 12/96\n",
      "2011/2011 [==============================] - 1s 250us/step - loss: 0.3216 - acc: 0.7001\n",
      "Epoch 13/96\n",
      "2011/2011 [==============================] - 1s 266us/step - loss: 0.3190 - acc: 0.7001\n",
      "Epoch 14/96\n",
      "2011/2011 [==============================] - 1s 255us/step - loss: 0.3169 - acc: 0.7001\n",
      "Epoch 15/96\n",
      "2011/2011 [==============================] - 0s 195us/step - loss: 0.3150 - acc: 0.7001\n",
      "Epoch 16/96\n",
      "2011/2011 [==============================] - 0s 211us/step - loss: 0.3134 - acc: 0.7001\n",
      "Epoch 17/96\n",
      "2011/2011 [==============================] - 0s 186us/step - loss: 0.3120 - acc: 0.7001\n",
      "Epoch 18/96\n",
      "2011/2011 [==============================] - 0s 205us/step - loss: 0.3108 - acc: 0.7001\n",
      "Epoch 19/96\n",
      "2011/2011 [==============================] - 0s 202us/step - loss: 0.3097 - acc: 0.7001\n",
      "Epoch 20/96\n",
      "2011/2011 [==============================] - 0s 221us/step - loss: 0.3088 - acc: 0.7001\n",
      "Epoch 21/96\n",
      "2011/2011 [==============================] - 0s 231us/step - loss: 0.3080 - acc: 0.7001\n",
      "Epoch 22/96\n",
      "2011/2011 [==============================] - 0s 188us/step - loss: 0.3072 - acc: 0.7001\n",
      "Epoch 23/96\n",
      "2011/2011 [==============================] - 0s 216us/step - loss: 0.3066 - acc: 0.7001\n",
      "Epoch 24/96\n",
      "2011/2011 [==============================] - 0s 190us/step - loss: 0.3060 - acc: 0.7001\n",
      "Epoch 25/96\n",
      "2011/2011 [==============================] - 0s 221us/step - loss: 0.3054 - acc: 0.7001\n",
      "Epoch 26/96\n",
      "2011/2011 [==============================] - 0s 211us/step - loss: 0.3050 - acc: 0.7001\n",
      "Epoch 27/96\n",
      "2011/2011 [==============================] - 0s 195us/step - loss: 0.3045 - acc: 0.7001\n",
      "Epoch 28/96\n",
      "2011/2011 [==============================] - 0s 214us/step - loss: 0.3041 - acc: 0.7001\n",
      "Epoch 29/96\n",
      "2011/2011 [==============================] - 1s 262us/step - loss: 0.3038 - acc: 0.7001\n",
      "Epoch 30/96\n",
      "2011/2011 [==============================] - 0s 182us/step - loss: 0.3035 - acc: 0.7001\n",
      "Epoch 31/96\n",
      "2011/2011 [==============================] - 0s 185us/step - loss: 0.3032 - acc: 0.7001\n",
      "Epoch 32/96\n",
      "2011/2011 [==============================] - 0s 189us/step - loss: 0.3029 - acc: 0.7001\n",
      "Epoch 33/96\n",
      "2011/2011 [==============================] - 0s 160us/step - loss: 0.3027 - acc: 0.7001\n",
      "Epoch 34/96\n",
      "2011/2011 [==============================] - 0s 165us/step - loss: 0.3024 - acc: 0.7001\n",
      "Epoch 35/96\n",
      "2011/2011 [==============================] - 0s 169us/step - loss: 0.3022 - acc: 0.7001\n",
      "Epoch 36/96\n",
      "2011/2011 [==============================] - 0s 159us/step - loss: 0.3021 - acc: 0.7001\n",
      "Epoch 37/96\n",
      "2011/2011 [==============================] - 0s 160us/step - loss: 0.3019 - acc: 0.7001\n",
      "Epoch 38/96\n",
      "2011/2011 [==============================] - 0s 166us/step - loss: 0.3017 - acc: 0.7001\n",
      "Epoch 39/96\n",
      "2011/2011 [==============================] - 0s 163us/step - loss: 0.3016 - acc: 0.7001\n",
      "Epoch 40/96\n",
      "2011/2011 [==============================] - 0s 165us/step - loss: 0.3014 - acc: 0.7001\n",
      "Epoch 41/96\n",
      "2011/2011 [==============================] - 0s 165us/step - loss: 0.3013 - acc: 0.7001\n",
      "Epoch 42/96\n",
      "2011/2011 [==============================] - 0s 153us/step - loss: 0.3012 - acc: 0.7001\n",
      "Epoch 43/96\n",
      "2011/2011 [==============================] - 0s 176us/step - loss: 0.3011 - acc: 0.7001\n",
      "Epoch 44/96\n",
      "2011/2011 [==============================] - 0s 176us/step - loss: 0.3010 - acc: 0.7001\n",
      "Epoch 45/96\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.3009 - acc: 0.7001\n",
      "Epoch 46/96\n",
      "2011/2011 [==============================] - 0s 192us/step - loss: 0.3008 - acc: 0.7001\n",
      "Epoch 47/96\n",
      "2011/2011 [==============================] - 0s 171us/step - loss: 0.3008 - acc: 0.7001\n",
      "Epoch 48/96\n",
      "2011/2011 [==============================] - 1s 259us/step - loss: 0.3007 - acc: 0.7001\n",
      "Epoch 49/96\n",
      "2011/2011 [==============================] - 1s 264us/step - loss: 0.3006 - acc: 0.7001\n",
      "Epoch 50/96\n",
      "2011/2011 [==============================] - 1s 438us/step - loss: 0.3006 - acc: 0.7001\n",
      "Epoch 51/96\n",
      "2011/2011 [==============================] - 1s 253us/step - loss: 0.3005 - acc: 0.7001\n",
      "Epoch 52/96\n",
      "2011/2011 [==============================] - 0s 149us/step - loss: 0.3005 - acc: 0.7001\n",
      "Epoch 53/96\n",
      "2011/2011 [==============================] - 0s 148us/step - loss: 0.3004 - acc: 0.7001\n",
      "Epoch 54/96\n",
      "2011/2011 [==============================] - 0s 152us/step - loss: 0.3004 - acc: 0.7001\n",
      "Epoch 55/96\n",
      "2011/2011 [==============================] - 0s 145us/step - loss: 0.3003 - acc: 0.7001\n",
      "Epoch 56/96\n",
      "2011/2011 [==============================] - 0s 162us/step - loss: 0.3003 - acc: 0.7001\n",
      "Epoch 57/96\n",
      "2011/2011 [==============================] - 0s 159us/step - loss: 0.3003 - acc: 0.7001\n",
      "Epoch 58/96\n",
      "2011/2011 [==============================] - 0s 173us/step - loss: 0.3002 - acc: 0.7001\n",
      "Epoch 59/96\n",
      "2011/2011 [==============================] - 0s 186us/step - loss: 0.3002 - acc: 0.7001\n",
      "Epoch 60/96\n",
      "2011/2011 [==============================] - 0s 177us/step - loss: 0.3002 - acc: 0.7001\n",
      "Epoch 61/96\n",
      "2011/2011 [==============================] - 0s 177us/step - loss: 0.3002 - acc: 0.7001\n",
      "Epoch 62/96\n",
      "2011/2011 [==============================] - 0s 162us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 63/96\n",
      "2011/2011 [==============================] - 0s 209us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 64/96\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 65/96\n",
      "2011/2011 [==============================] - 0s 199us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 66/96\n",
      "2011/2011 [==============================] - 0s 138us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 67/96\n",
      "2011/2011 [==============================] - 0s 137us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 68/96\n",
      "2011/2011 [==============================] - 0s 135us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 69/96\n",
      "2011/2011 [==============================] - 0s 137us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 70/96\n",
      "2011/2011 [==============================] - 0s 135us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 71/96\n",
      "2011/2011 [==============================] - 0s 135us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 72/96\n",
      "2011/2011 [==============================] - 0s 131us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 73/96\n",
      "2011/2011 [==============================] - 0s 137us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 74/96\n",
      "2011/2011 [==============================] - 0s 133us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 75/96\n",
      "2011/2011 [==============================] - 0s 132us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 76/96\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 77/96\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 78/96\n",
      "2011/2011 [==============================] - 0s 130us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 79/96\n",
      "2011/2011 [==============================] - 0s 129us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 80/96\n",
      "2011/2011 [==============================] - 0s 137us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 81/96\n",
      "2011/2011 [==============================] - 0s 135us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 82/96\n",
      "2011/2011 [==============================] - 0s 134us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 83/96\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 84/96\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 85/96\n",
      "2011/2011 [==============================] - 0s 131us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 86/96\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 87/96\n",
      "2011/2011 [==============================] - 0s 134us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 88/96\n",
      "2011/2011 [==============================] - 0s 132us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 89/96\n",
      "2011/2011 [==============================] - 0s 130us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 90/96\n",
      "2011/2011 [==============================] - 0s 133us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 91/96\n",
      "2011/2011 [==============================] - 0s 134us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 92/96\n",
      "2011/2011 [==============================] - 0s 135us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 93/96\n",
      "2011/2011 [==============================] - 0s 130us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 94/96\n",
      "2011/2011 [==============================] - 0s 126us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 95/96\n",
      "2011/2011 [==============================] - 0s 137us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 96/96\n",
      "2011/2011 [==============================] - 0s 132us/step - loss: 0.2999 - acc: 0.7001\n",
      "34.73250126838684\n",
      "{\"cols:['diff_bin'], bs:12, ep:96 (cm)\": array([[24, 22],\n",
      "       [13, 47]]), \"cols:['diff_bin'], bs:12, ep:96 (accuracy)\": '67.0%'}\n",
      "Epoch 1/196\n",
      "2011/2011 [==============================] - 1s 375us/step - loss: 0.4836 - acc: 0.6017\n",
      "Epoch 2/196\n",
      "2011/2011 [==============================] - 0s 139us/step - loss: 0.4203 - acc: 0.6917\n",
      "Epoch 3/196\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3945 - acc: 0.6907\n",
      "Epoch 4/196\n",
      "2011/2011 [==============================] - 0s 139us/step - loss: 0.3771 - acc: 0.6927\n",
      "Epoch 5/196\n",
      "2011/2011 [==============================] - 0s 134us/step - loss: 0.3661 - acc: 0.6912\n",
      "Epoch 6/196\n",
      "2011/2011 [==============================] - 0s 132us/step - loss: 0.3561 - acc: 0.6952\n",
      "Epoch 7/196\n",
      "2011/2011 [==============================] - 0s 138us/step - loss: 0.3478 - acc: 0.6962\n",
      "Epoch 8/196\n",
      "2011/2011 [==============================] - 0s 137us/step - loss: 0.3403 - acc: 0.6982\n",
      "Epoch 9/196\n",
      "2011/2011 [==============================] - 0s 137us/step - loss: 0.3345 - acc: 0.7001\n",
      "Epoch 10/196\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3298 - acc: 0.7001\n",
      "Epoch 11/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3259 - acc: 0.7001\n",
      "Epoch 12/196\n",
      "2011/2011 [==============================] - 0s 157us/step - loss: 0.3227 - acc: 0.7001\n",
      "Epoch 13/196\n",
      "2011/2011 [==============================] - 0s 140us/step - loss: 0.3200 - acc: 0.7001\n",
      "Epoch 14/196\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3177 - acc: 0.7001\n",
      "Epoch 15/196\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3157 - acc: 0.7001\n",
      "Epoch 16/196\n",
      "2011/2011 [==============================] - 0s 138us/step - loss: 0.3141 - acc: 0.7001\n",
      "Epoch 17/196\n",
      "2011/2011 [==============================] - 0s 139us/step - loss: 0.3126 - acc: 0.7001\n",
      "Epoch 18/196\n",
      "2011/2011 [==============================] - 0s 140us/step - loss: 0.3113 - acc: 0.7001\n",
      "Epoch 19/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3102 - acc: 0.7001\n",
      "Epoch 20/196\n",
      "2011/2011 [==============================] - 0s 143us/step - loss: 0.3092 - acc: 0.7001\n",
      "Epoch 21/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3083 - acc: 0.7001\n",
      "Epoch 22/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3076 - acc: 0.7001\n",
      "Epoch 23/196\n",
      "2011/2011 [==============================] - 0s 140us/step - loss: 0.3069 - acc: 0.7001\n",
      "Epoch 24/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3063 - acc: 0.7001\n",
      "Epoch 25/196\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3057 - acc: 0.7001\n",
      "Epoch 26/196\n",
      "2011/2011 [==============================] - 0s 137us/step - loss: 0.3052 - acc: 0.7001\n",
      "Epoch 27/196\n",
      "2011/2011 [==============================] - 0s 140us/step - loss: 0.3047 - acc: 0.7001\n",
      "Epoch 28/196\n",
      "2011/2011 [==============================] - 0s 138us/step - loss: 0.3043 - acc: 0.7001\n",
      "Epoch 29/196\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3040 - acc: 0.7001\n",
      "Epoch 30/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3036 - acc: 0.7001\n",
      "Epoch 31/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3033 - acc: 0.7001\n",
      "Epoch 32/196\n",
      "2011/2011 [==============================] - 0s 149us/step - loss: 0.3030 - acc: 0.7001\n",
      "Epoch 33/196\n",
      "2011/2011 [==============================] - 0s 139us/step - loss: 0.3028 - acc: 0.7001\n",
      "Epoch 34/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3026 - acc: 0.7001\n",
      "Epoch 35/196\n",
      "2011/2011 [==============================] - 0s 142us/step - loss: 0.3023 - acc: 0.7001\n",
      "Epoch 36/196\n",
      "2011/2011 [==============================] - 0s 143us/step - loss: 0.3021 - acc: 0.7001\n",
      "Epoch 37/196\n",
      "2011/2011 [==============================] - 0s 135us/step - loss: 0.3020 - acc: 0.7001\n",
      "Epoch 38/196\n",
      "2011/2011 [==============================] - 0s 143us/step - loss: 0.3018 - acc: 0.7001\n",
      "Epoch 39/196\n",
      "2011/2011 [==============================] - 0s 140us/step - loss: 0.3017 - acc: 0.7001\n",
      "Epoch 40/196\n",
      "2011/2011 [==============================] - 0s 143us/step - loss: 0.3015 - acc: 0.7001\n",
      "Epoch 41/196\n",
      "2011/2011 [==============================] - 0s 146us/step - loss: 0.3014 - acc: 0.7001\n",
      "Epoch 42/196\n",
      "2011/2011 [==============================] - 0s 139us/step - loss: 0.3013 - acc: 0.7001\n",
      "Epoch 43/196\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3012 - acc: 0.7001\n",
      "Epoch 44/196\n",
      "2011/2011 [==============================] - 0s 136us/step - loss: 0.3011 - acc: 0.7001\n",
      "Epoch 45/196\n",
      "2011/2011 [==============================] - 0s 140us/step - loss: 0.3010 - acc: 0.7001\n",
      "Epoch 46/196\n",
      "2011/2011 [==============================] - 0s 141us/step - loss: 0.3009 - acc: 0.7001\n",
      "Epoch 47/196\n",
      "2011/2011 [==============================] - 0s 147us/step - loss: 0.3008 - acc: 0.7001\n",
      "Epoch 48/196\n",
      "2011/2011 [==============================] - 0s 143us/step - loss: 0.3007 - acc: 0.7001\n",
      "Epoch 49/196\n",
      "2011/2011 [==============================] - 0s 143us/step - loss: 0.3007 - acc: 0.7001\n",
      "Epoch 50/196\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.3006 - acc: 0.7001\n",
      "Epoch 51/196\n",
      "2011/2011 [==============================] - 0s 214us/step - loss: 0.3006 - acc: 0.7001\n",
      "Epoch 52/196\n",
      "2011/2011 [==============================] - 0s 163us/step - loss: 0.3005 - acc: 0.7001\n",
      "Epoch 53/196\n",
      "2011/2011 [==============================] - 0s 211us/step - loss: 0.3005 - acc: 0.7001\n",
      "Epoch 54/196\n",
      "2011/2011 [==============================] - 0s 194us/step - loss: 0.3004 - acc: 0.7001\n",
      "Epoch 55/196\n",
      "2011/2011 [==============================] - 0s 194us/step - loss: 0.3004 - acc: 0.7001\n",
      "Epoch 56/196\n",
      "2011/2011 [==============================] - 0s 177us/step - loss: 0.3003 - acc: 0.7001\n",
      "Epoch 57/196\n",
      "2011/2011 [==============================] - 0s 196us/step - loss: 0.3003 - acc: 0.7001\n",
      "Epoch 58/196\n",
      "2011/2011 [==============================] - 0s 198us/step - loss: 0.3003 - acc: 0.7001\n",
      "Epoch 59/196\n",
      "2011/2011 [==============================] - 1s 283us/step - loss: 0.3002 - acc: 0.7001\n",
      "Epoch 60/196\n",
      "2011/2011 [==============================] - 0s 224us/step - loss: 0.3002 - acc: 0.7001\n",
      "Epoch 61/196\n",
      "2011/2011 [==============================] - 0s 215us/step - loss: 0.3002 - acc: 0.7001\n",
      "Epoch 62/196\n",
      "2011/2011 [==============================] - 0s 196us/step - loss: 0.3002 - acc: 0.7001\n",
      "Epoch 63/196\n",
      "2011/2011 [==============================] - 0s 209us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 64/196\n",
      "2011/2011 [==============================] - 0s 209us/step - loss: 0.3001 - acc: 0.7001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/196\n",
      "2011/2011 [==============================] - 0s 204us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 66/196\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 67/196\n",
      "2011/2011 [==============================] - 0s 178us/step - loss: 0.3001 - acc: 0.7001\n",
      "Epoch 68/196\n",
      "2011/2011 [==============================] - 0s 185us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 69/196\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 70/196\n",
      "2011/2011 [==============================] - 0s 200us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 71/196\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 72/196\n",
      "2011/2011 [==============================] - 0s 173us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 73/196\n",
      "2011/2011 [==============================] - 0s 154us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 74/196\n",
      "2011/2011 [==============================] - 0s 161us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 75/196\n",
      "2011/2011 [==============================] - 0s 158us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 76/196\n",
      "2011/2011 [==============================] - 0s 168us/step - loss: 0.3000 - acc: 0.7001\n",
      "Epoch 77/196\n",
      "2011/2011 [==============================] - 0s 163us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 78/196\n",
      "2011/2011 [==============================] - 0s 166us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 79/196\n",
      "2011/2011 [==============================] - 0s 152us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 80/196\n",
      "2011/2011 [==============================] - 0s 167us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 81/196\n",
      "2011/2011 [==============================] - 0s 158us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 82/196\n",
      "2011/2011 [==============================] - 0s 160us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 83/196\n",
      "2011/2011 [==============================] - 0s 163us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 84/196\n",
      "2011/2011 [==============================] - 0s 164us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 85/196\n",
      "2011/2011 [==============================] - 0s 166us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 86/196\n",
      "2011/2011 [==============================] - 0s 152us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 87/196\n",
      "2011/2011 [==============================] - 0s 157us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 88/196\n",
      "2011/2011 [==============================] - 0s 164us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 89/196\n",
      "2011/2011 [==============================] - 0s 154us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 90/196\n",
      "2011/2011 [==============================] - 0s 156us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 91/196\n",
      "2011/2011 [==============================] - 0s 160us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 92/196\n",
      "2011/2011 [==============================] - 0s 218us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 93/196\n",
      "2011/2011 [==============================] - 0s 226us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 94/196\n",
      "2011/2011 [==============================] - 0s 185us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 95/196\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 96/196\n",
      "2011/2011 [==============================] - 0s 206us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 97/196\n",
      "2011/2011 [==============================] - 0s 176us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 98/196\n",
      "2011/2011 [==============================] - 0s 215us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 99/196\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 100/196\n",
      "2011/2011 [==============================] - 0s 203us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 101/196\n",
      "2011/2011 [==============================] - 0s 188us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 102/196\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 103/196\n",
      "2011/2011 [==============================] - 0s 180us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 104/196\n",
      "2011/2011 [==============================] - 0s 170us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 105/196\n",
      "2011/2011 [==============================] - 0s 171us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 106/196\n",
      "2011/2011 [==============================] - 0s 176us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 107/196\n",
      "2011/2011 [==============================] - 0s 187us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 108/196\n",
      "2011/2011 [==============================] - 0s 184us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 109/196\n",
      "2011/2011 [==============================] - 0s 246us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 110/196\n",
      "2011/2011 [==============================] - 0s 177us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 111/196\n",
      "2011/2011 [==============================] - 0s 168us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 112/196\n",
      "2011/2011 [==============================] - 0s 189us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 113/196\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 114/196\n",
      "2011/2011 [==============================] - 0s 187us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 115/196\n",
      "2011/2011 [==============================] - 0s 187us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 116/196\n",
      "2011/2011 [==============================] - 0s 180us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 117/196\n",
      "2011/2011 [==============================] - 0s 187us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 118/196\n",
      "2011/2011 [==============================] - 0s 185us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 119/196\n",
      "2011/2011 [==============================] - 0s 181us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 120/196\n",
      "2011/2011 [==============================] - 0s 183us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 121/196\n",
      "2011/2011 [==============================] - 0s 190us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 122/196\n",
      "2011/2011 [==============================] - 0s 188us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 123/196\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 124/196\n",
      "2011/2011 [==============================] - 0s 186us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 125/196\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 126/196\n",
      "2011/2011 [==============================] - 0s 183us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 127/196\n",
      "2011/2011 [==============================] - 0s 192us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 128/196\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 129/196\n",
      "2011/2011 [==============================] - 0s 176us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 130/196\n",
      "2011/2011 [==============================] - 0s 176us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 131/196\n",
      "2011/2011 [==============================] - 0s 199us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 132/196\n",
      "2011/2011 [==============================] - 0s 181us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 133/196\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 134/196\n",
      "2011/2011 [==============================] - 0s 171us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 135/196\n",
      "2011/2011 [==============================] - 0s 182us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 136/196\n",
      "2011/2011 [==============================] - 0s 192us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 137/196\n",
      "2011/2011 [==============================] - 0s 161us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 138/196\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 139/196\n",
      "2011/2011 [==============================] - 0s 189us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 140/196\n",
      "2011/2011 [==============================] - 0s 183us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 141/196\n",
      "2011/2011 [==============================] - 0s 196us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 142/196\n",
      "2011/2011 [==============================] - 0s 168us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 143/196\n",
      "2011/2011 [==============================] - 0s 169us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 144/196\n",
      "2011/2011 [==============================] - 0s 178us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 145/196\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 146/196\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 147/196\n",
      "2011/2011 [==============================] - 0s 196us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 148/196\n",
      "2011/2011 [==============================] - 0s 178us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 149/196\n",
      "2011/2011 [==============================] - 0s 159us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 150/196\n",
      "2011/2011 [==============================] - 0s 176us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 151/196\n",
      "2011/2011 [==============================] - 0s 197us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 152/196\n",
      "2011/2011 [==============================] - 0s 168us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 153/196\n",
      "2011/2011 [==============================] - 0s 186us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 154/196\n",
      "2011/2011 [==============================] - 0s 158us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 155/196\n",
      "2011/2011 [==============================] - 0s 166us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 156/196\n",
      "2011/2011 [==============================] - 0s 183us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 157/196\n",
      "2011/2011 [==============================] - 0s 167us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 158/196\n",
      "2011/2011 [==============================] - 0s 171us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 159/196\n",
      "2011/2011 [==============================] - 0s 173us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 160/196\n",
      "2011/2011 [==============================] - 0s 163us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 161/196\n",
      "2011/2011 [==============================] - 0s 169us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 162/196\n",
      "2011/2011 [==============================] - 0s 171us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 163/196\n",
      "2011/2011 [==============================] - 0s 177us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 164/196\n",
      "2011/2011 [==============================] - 0s 171us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 165/196\n",
      "2011/2011 [==============================] - 0s 171us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 166/196\n",
      "2011/2011 [==============================] - 0s 161us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 167/196\n",
      "2011/2011 [==============================] - 0s 170us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 168/196\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 169/196\n",
      "2011/2011 [==============================] - 0s 167us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 170/196\n",
      "2011/2011 [==============================] - 0s 167us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 171/196\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 172/196\n",
      "2011/2011 [==============================] - 0s 165us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 173/196\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 174/196\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 175/196\n",
      "2011/2011 [==============================] - 0s 185us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 176/196\n",
      "2011/2011 [==============================] - 0s 173us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 177/196\n",
      "2011/2011 [==============================] - 0s 168us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 178/196\n",
      "2011/2011 [==============================] - 0s 166us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 179/196\n",
      "2011/2011 [==============================] - 0s 163us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 180/196\n",
      "2011/2011 [==============================] - 0s 175us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 181/196\n",
      "2011/2011 [==============================] - 0s 176us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 182/196\n",
      "2011/2011 [==============================] - 0s 157us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 183/196\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 184/196\n",
      "2011/2011 [==============================] - 0s 169us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 185/196\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 186/196\n",
      "2011/2011 [==============================] - 0s 168us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 187/196\n",
      "2011/2011 [==============================] - 0s 174us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 188/196\n",
      "2011/2011 [==============================] - 0s 172us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 189/196\n",
      "2011/2011 [==============================] - 0s 173us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 190/196\n",
      "2011/2011 [==============================] - 0s 181us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 191/196\n",
      "2011/2011 [==============================] - 0s 175us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 192/196\n",
      "2011/2011 [==============================] - 0s 179us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 193/196\n",
      "2011/2011 [==============================] - 0s 164us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 194/196\n",
      "2011/2011 [==============================] - 0s 171us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 195/196\n",
      "2011/2011 [==============================] - 0s 178us/step - loss: 0.2999 - acc: 0.7001\n",
      "Epoch 196/196\n",
      "2011/2011 [==============================] - 0s 185us/step - loss: 0.2999 - acc: 0.7001\n",
      "102.49924898147583\n",
      "{\"cols:['diff_bin'], bs:12, ep:96 (cm)\": array([[24, 22],\n",
      "       [13, 47]]), \"cols:['diff_bin'], bs:12, ep:96 (accuracy)\": '67.0%', \"cols:['diff_bin'], bs:12, ep:196 (cm)\": array([[24, 22],\n",
      "       [13, 47]]), \"cols:['diff_bin'], bs:12, ep:196 (accuracy)\": '67.0%'}\n",
      "Epoch 1/96\n",
      "2011/2011 [==============================] - 1s 384us/step - loss: 0.4963 - acc: 0.5917\n",
      "Epoch 2/96\n",
      "2011/2011 [==============================] - 0s 101us/step - loss: 0.4525 - acc: 0.6957\n",
      "Epoch 3/96\n",
      "2011/2011 [==============================] - 0s 100us/step - loss: 0.4197 - acc: 0.6912\n",
      "Epoch 4/96\n",
      "2011/2011 [==============================] - 0s 95us/step - loss: 0.4033 - acc: 0.6957\n",
      "Epoch 5/96\n",
      "2011/2011 [==============================] - 0s 111us/step - loss: 0.3913 - acc: 0.6952\n",
      "Epoch 6/96\n",
      "2011/2011 [==============================] - 0s 113us/step - loss: 0.3817 - acc: 0.6942\n",
      "Epoch 7/96\n",
      "2011/2011 [==============================] - 0s 99us/step - loss: 0.3738 - acc: 0.6927\n",
      "Epoch 8/96\n",
      "2011/2011 [==============================] - 0s 97us/step - loss: 0.3672 - acc: 0.6942\n",
      "Epoch 9/96\n",
      "2011/2011 [==============================] - 0s 98us/step - loss: 0.3615 - acc: 0.6942\n",
      "Epoch 10/96\n",
      "2011/2011 [==============================] - 0s 107us/step - loss: 0.3566 - acc: 0.6922\n",
      "Epoch 11/96\n",
      "2011/2011 [==============================] - 0s 101us/step - loss: 0.3518 - acc: 0.6917\n",
      "Epoch 12/96\n",
      "2011/2011 [==============================] - 0s 91us/step - loss: 0.3477 - acc: 0.6922\n",
      "Epoch 13/96\n",
      "2011/2011 [==============================] - 0s 98us/step - loss: 0.3438 - acc: 0.6932\n",
      "Epoch 14/96\n",
      "2011/2011 [==============================] - 0s 96us/step - loss: 0.3409 - acc: 0.6922\n",
      "Epoch 15/96\n",
      "2011/2011 [==============================] - 0s 96us/step - loss: 0.3372 - acc: 0.6957\n",
      "Epoch 16/96\n",
      "2011/2011 [==============================] - 0s 100us/step - loss: 0.3364 - acc: 0.6907\n",
      "Epoch 17/96\n",
      "2011/2011 [==============================] - 0s 103us/step - loss: 0.3322 - acc: 0.6957\n",
      "Epoch 18/96\n",
      "2011/2011 [==============================] - 0s 100us/step - loss: 0.3304 - acc: 0.6947\n",
      "Epoch 19/96\n",
      "2011/2011 [==============================] - 0s 103us/step - loss: 0.3285 - acc: 0.6947\n",
      "Epoch 20/96\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3265 - acc: 0.6957\n",
      "Epoch 21/96\n",
      "2011/2011 [==============================] - 0s 100us/step - loss: 0.3248 - acc: 0.6967\n",
      "Epoch 22/96\n",
      "2011/2011 [==============================] - 0s 99us/step - loss: 0.3233 - acc: 0.6967\n",
      "Epoch 23/96\n",
      "2011/2011 [==============================] - 0s 102us/step - loss: 0.3220 - acc: 0.6967\n",
      "Epoch 24/96\n",
      "2011/2011 [==============================] - 0s 98us/step - loss: 0.3207 - acc: 0.6967\n",
      "Epoch 25/96\n",
      "2011/2011 [==============================] - 0s 92us/step - loss: 0.3196 - acc: 0.6967\n",
      "Epoch 26/96\n",
      "2011/2011 [==============================] - 0s 101us/step - loss: 0.3186 - acc: 0.6967\n",
      "Epoch 27/96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011/2011 [==============================] - 0s 95us/step - loss: 0.3176 - acc: 0.6967\n",
      "Epoch 28/96\n",
      "2011/2011 [==============================] - 0s 82us/step - loss: 0.3167 - acc: 0.6967\n",
      "Epoch 29/96\n",
      "2011/2011 [==============================] - 0s 78us/step - loss: 0.3158 - acc: 0.6967\n",
      "Epoch 30/96\n",
      "2011/2011 [==============================] - 0s 90us/step - loss: 0.3150 - acc: 0.6967\n",
      "Epoch 31/96\n",
      "2011/2011 [==============================] - 0s 81us/step - loss: 0.3140 - acc: 0.6967\n",
      "Epoch 32/96\n",
      "2011/2011 [==============================] - 0s 83us/step - loss: 0.3132 - acc: 0.6977\n",
      "Epoch 33/96\n",
      "2011/2011 [==============================] - 0s 85us/step - loss: 0.3124 - acc: 0.6977\n",
      "Epoch 34/96\n",
      "2011/2011 [==============================] - 0s 78us/step - loss: 0.3117 - acc: 0.6977\n",
      "Epoch 35/96\n",
      "2011/2011 [==============================] - 0s 77us/step - loss: 0.3111 - acc: 0.6977\n",
      "Epoch 36/96\n",
      "2011/2011 [==============================] - 0s 77us/step - loss: 0.3106 - acc: 0.6987\n",
      "Epoch 37/96\n",
      "2011/2011 [==============================] - 0s 94us/step - loss: 0.3100 - acc: 0.6987\n",
      "Epoch 38/96\n",
      "2011/2011 [==============================] - 0s 90us/step - loss: 0.3096 - acc: 0.6987\n",
      "Epoch 39/96\n",
      "2011/2011 [==============================] - 0s 87us/step - loss: 0.3091 - acc: 0.6987\n",
      "Epoch 40/96\n",
      "2011/2011 [==============================] - 0s 83us/step - loss: 0.3087 - acc: 0.6987\n",
      "Epoch 41/96\n",
      "2011/2011 [==============================] - 0s 78us/step - loss: 0.3083 - acc: 0.6987\n",
      "Epoch 42/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3080 - acc: 0.6987\n",
      "Epoch 43/96\n",
      "2011/2011 [==============================] - 0s 87us/step - loss: 0.3076 - acc: 0.6987\n",
      "Epoch 44/96\n",
      "2011/2011 [==============================] - 0s 76us/step - loss: 0.3073 - acc: 0.6987\n",
      "Epoch 45/96\n",
      "2011/2011 [==============================] - 0s 92us/step - loss: 0.3070 - acc: 0.6987\n",
      "Epoch 46/96\n",
      "2011/2011 [==============================] - 0s 85us/step - loss: 0.3068 - acc: 0.6987\n",
      "Epoch 47/96\n",
      "2011/2011 [==============================] - 0s 82us/step - loss: 0.3065 - acc: 0.6987\n",
      "Epoch 48/96\n",
      "2011/2011 [==============================] - 0s 82us/step - loss: 0.3063 - acc: 0.6987\n",
      "Epoch 49/96\n",
      "2011/2011 [==============================] - 0s 95us/step - loss: 0.3060 - acc: 0.6987\n",
      "Epoch 50/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3058 - acc: 0.6987\n",
      "Epoch 51/96\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3056 - acc: 0.6987\n",
      "Epoch 52/96\n",
      "2011/2011 [==============================] - 0s 85us/step - loss: 0.3054 - acc: 0.6987\n",
      "Epoch 53/96\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3053 - acc: 0.6987\n",
      "Epoch 54/96\n",
      "2011/2011 [==============================] - 0s 92us/step - loss: 0.3051 - acc: 0.6987\n",
      "Epoch 55/96\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3049 - acc: 0.6987\n",
      "Epoch 56/96\n",
      "2011/2011 [==============================] - 0s 76us/step - loss: 0.3048 - acc: 0.6987\n",
      "Epoch 57/96\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3046 - acc: 0.6987\n",
      "Epoch 58/96\n",
      "2011/2011 [==============================] - 0s 85us/step - loss: 0.3045 - acc: 0.6987\n",
      "Epoch 59/96\n",
      "2011/2011 [==============================] - 0s 81us/step - loss: 0.3043 - acc: 0.6987\n",
      "Epoch 60/96\n",
      "2011/2011 [==============================] - 0s 92us/step - loss: 0.3042 - acc: 0.6987\n",
      "Epoch 61/96\n",
      "2011/2011 [==============================] - 0s 95us/step - loss: 0.3041 - acc: 0.6987\n",
      "Epoch 62/96\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3040 - acc: 0.6987\n",
      "Epoch 63/96\n",
      "2011/2011 [==============================] - 0s 88us/step - loss: 0.3039 - acc: 0.6987\n",
      "Epoch 64/96\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3038 - acc: 0.6987\n",
      "Epoch 65/96\n",
      "2011/2011 [==============================] - 0s 90us/step - loss: 0.3036 - acc: 0.6987\n",
      "Epoch 66/96\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3036 - acc: 0.6987\n",
      "Epoch 67/96\n",
      "2011/2011 [==============================] - 0s 87us/step - loss: 0.3035 - acc: 0.6987\n",
      "Epoch 68/96\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3034 - acc: 0.6987\n",
      "Epoch 69/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3033 - acc: 0.6987\n",
      "Epoch 70/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3032 - acc: 0.6987\n",
      "Epoch 71/96\n",
      "2011/2011 [==============================] - 0s 96us/step - loss: 0.3031 - acc: 0.6987\n",
      "Epoch 72/96\n",
      "2011/2011 [==============================] - 0s 85us/step - loss: 0.3031 - acc: 0.6987\n",
      "Epoch 73/96\n",
      "2011/2011 [==============================] - 0s 96us/step - loss: 0.3030 - acc: 0.6987\n",
      "Epoch 74/96\n",
      "2011/2011 [==============================] - 0s 90us/step - loss: 0.3029 - acc: 0.6987\n",
      "Epoch 75/96\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3029 - acc: 0.6987\n",
      "Epoch 76/96\n",
      "2011/2011 [==============================] - 0s 92us/step - loss: 0.3028 - acc: 0.6987\n",
      "Epoch 77/96\n",
      "2011/2011 [==============================] - 0s 81us/step - loss: 0.3027 - acc: 0.6987\n",
      "Epoch 78/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3027 - acc: 0.6987\n",
      "Epoch 79/96\n",
      "2011/2011 [==============================] - 0s 80us/step - loss: 0.3026 - acc: 0.6987\n",
      "Epoch 80/96\n",
      "2011/2011 [==============================] - 0s 78us/step - loss: 0.3026 - acc: 0.6987\n",
      "Epoch 81/96\n",
      "2011/2011 [==============================] - 0s 81us/step - loss: 0.3025 - acc: 0.6987\n",
      "Epoch 82/96\n",
      "2011/2011 [==============================] - 0s 81us/step - loss: 0.3025 - acc: 0.6987\n",
      "Epoch 83/96\n",
      "2011/2011 [==============================] - 0s 90us/step - loss: 0.3024 - acc: 0.6987\n",
      "Epoch 84/96\n",
      "2011/2011 [==============================] - 0s 74us/step - loss: 0.3024 - acc: 0.6987\n",
      "Epoch 85/96\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3024 - acc: 0.6987\n",
      "Epoch 86/96\n",
      "2011/2011 [==============================] - 0s 82us/step - loss: 0.3023 - acc: 0.6987\n",
      "Epoch 87/96\n",
      "2011/2011 [==============================] - 0s 77us/step - loss: 0.3023 - acc: 0.6987\n",
      "Epoch 88/96\n",
      "2011/2011 [==============================] - 0s 88us/step - loss: 0.3022 - acc: 0.6987\n",
      "Epoch 89/96\n",
      "2011/2011 [==============================] - 0s 88us/step - loss: 0.3022 - acc: 0.6987\n",
      "Epoch 90/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3022 - acc: 0.6987\n",
      "Epoch 91/96\n",
      "2011/2011 [==============================] - 0s 87us/step - loss: 0.3021 - acc: 0.6987\n",
      "Epoch 92/96\n",
      "2011/2011 [==============================] - 0s 87us/step - loss: 0.3021 - acc: 0.6987\n",
      "Epoch 93/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3021 - acc: 0.6987\n",
      "Epoch 94/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3020 - acc: 0.6987\n",
      "Epoch 95/96\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3020 - acc: 0.6987\n",
      "Epoch 96/96\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3020 - acc: 0.6987\n",
      "120.88299632072449\n",
      "{\"cols:['diff_bin'], bs:12, ep:96 (cm)\": array([[24, 22],\n",
      "       [13, 47]]), \"cols:['diff_bin'], bs:12, ep:96 (accuracy)\": '67.0%', \"cols:['diff_bin'], bs:12, ep:196 (cm)\": array([[24, 22],\n",
      "       [13, 47]]), \"cols:['diff_bin'], bs:12, ep:196 (accuracy)\": '67.0%', \"cols:['diff_bin'], bs:24, ep:96 (cm)\": array([[36, 10],\n",
      "       [27, 33]]), \"cols:['diff_bin'], bs:24, ep:96 (accuracy)\": '65.1%'}\n",
      "Epoch 1/196\n",
      "2011/2011 [==============================] - 1s 377us/step - loss: 0.4950 - acc: 0.5619\n",
      "Epoch 2/196\n",
      "2011/2011 [==============================] - 0s 93us/step - loss: 0.4483 - acc: 0.6947\n",
      "Epoch 3/196\n",
      "2011/2011 [==============================] - 0s 96us/step - loss: 0.4168 - acc: 0.6917\n",
      "Epoch 4/196\n",
      "2011/2011 [==============================] - 0s 88us/step - loss: 0.4018 - acc: 0.6957\n",
      "Epoch 5/196\n",
      "2011/2011 [==============================] - 0s 92us/step - loss: 0.3904 - acc: 0.6952\n",
      "Epoch 6/196\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3812 - acc: 0.6932\n",
      "Epoch 7/196\n",
      "2011/2011 [==============================] - 0s 90us/step - loss: 0.3735 - acc: 0.6927\n",
      "Epoch 8/196\n",
      "2011/2011 [==============================] - 0s 81us/step - loss: 0.3670 - acc: 0.6942\n",
      "Epoch 9/196\n",
      "2011/2011 [==============================] - 0s 82us/step - loss: 0.3615 - acc: 0.6932\n",
      "Epoch 10/196\n",
      "2011/2011 [==============================] - 0s 82us/step - loss: 0.3566 - acc: 0.6922\n",
      "Epoch 11/196\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3519 - acc: 0.6917\n",
      "Epoch 12/196\n",
      "2011/2011 [==============================] - 0s 88us/step - loss: 0.3477 - acc: 0.6922\n",
      "Epoch 13/196\n",
      "2011/2011 [==============================] - 0s 87us/step - loss: 0.3440 - acc: 0.6937\n",
      "Epoch 14/196\n",
      "2011/2011 [==============================] - 0s 83us/step - loss: 0.3407 - acc: 0.6927\n",
      "Epoch 15/196\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3377 - acc: 0.6932\n",
      "Epoch 16/196\n",
      "2011/2011 [==============================] - 0s 83us/step - loss: 0.3353 - acc: 0.6922\n",
      "Epoch 17/196\n",
      "2011/2011 [==============================] - 0s 88us/step - loss: 0.3326 - acc: 0.6952\n",
      "Epoch 18/196\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3307 - acc: 0.6942\n",
      "Epoch 19/196\n",
      "2011/2011 [==============================] - 0s 83us/step - loss: 0.3287 - acc: 0.6947\n",
      "Epoch 20/196\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3267 - acc: 0.6957\n",
      "Epoch 21/196\n",
      "2011/2011 [==============================] - 0s 93us/step - loss: 0.3249 - acc: 0.6967\n",
      "Epoch 22/196\n",
      "2011/2011 [==============================] - 0s 87us/step - loss: 0.3234 - acc: 0.6967\n",
      "Epoch 23/196\n",
      "2011/2011 [==============================] - 0s 148us/step - loss: 0.3221 - acc: 0.6967\n",
      "Epoch 24/196\n",
      "2011/2011 [==============================] - 0s 123us/step - loss: 0.3209 - acc: 0.6967\n",
      "Epoch 25/196\n",
      "2011/2011 [==============================] - 0s 84us/step - loss: 0.3197 - acc: 0.6967\n",
      "Epoch 26/196\n",
      "2011/2011 [==============================] - 0s 95us/step - loss: 0.3187 - acc: 0.6967\n",
      "Epoch 27/196\n",
      "2011/2011 [==============================] - 0s 123us/step - loss: 0.3177 - acc: 0.6967\n",
      "Epoch 28/196\n",
      "2011/2011 [==============================] - 0s 95us/step - loss: 0.3168 - acc: 0.6967\n",
      "Epoch 29/196\n",
      "2011/2011 [==============================] - 0s 99us/step - loss: 0.3159 - acc: 0.6967\n",
      "Epoch 30/196\n",
      "2011/2011 [==============================] - 0s 115us/step - loss: 0.3150 - acc: 0.6967\n",
      "Epoch 31/196\n",
      "2011/2011 [==============================] - 0s 95us/step - loss: 0.3141 - acc: 0.6967\n",
      "Epoch 32/196\n",
      "2011/2011 [==============================] - 0s 111us/step - loss: 0.3132 - acc: 0.6977\n",
      "Epoch 33/196\n",
      "2011/2011 [==============================] - 0s 95us/step - loss: 0.3124 - acc: 0.6977\n",
      "Epoch 34/196\n",
      "2011/2011 [==============================] - 0s 103us/step - loss: 0.3118 - acc: 0.6977\n",
      "Epoch 35/196\n",
      "2011/2011 [==============================] - 0s 93us/step - loss: 0.3112 - acc: 0.6977\n",
      "Epoch 36/196\n",
      "2011/2011 [==============================] - 0s 78us/step - loss: 0.3106 - acc: 0.6987\n",
      "Epoch 37/196\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3101 - acc: 0.6987\n",
      "Epoch 38/196\n",
      "2011/2011 [==============================] - 0s 82us/step - loss: 0.3096 - acc: 0.6987\n",
      "Epoch 39/196\n",
      "2011/2011 [==============================] - 0s 90us/step - loss: 0.3092 - acc: 0.6987\n",
      "Epoch 40/196\n",
      "2011/2011 [==============================] - 0s 92us/step - loss: 0.3087 - acc: 0.6987\n",
      "Epoch 41/196\n",
      "2011/2011 [==============================] - 0s 86us/step - loss: 0.3084 - acc: 0.6987\n",
      "Epoch 42/196\n",
      "2011/2011 [==============================] - 0s 80us/step - loss: 0.3080 - acc: 0.6987\n",
      "Epoch 43/196\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3077 - acc: 0.6987\n",
      "Epoch 44/196\n",
      "2011/2011 [==============================] - 0s 112us/step - loss: 0.3074 - acc: 0.6987\n",
      "Epoch 45/196\n",
      "2011/2011 [==============================] - 0s 110us/step - loss: 0.3071 - acc: 0.6987\n",
      "Epoch 46/196\n",
      "2011/2011 [==============================] - 0s 94us/step - loss: 0.3068 - acc: 0.6987\n",
      "Epoch 47/196\n",
      "2011/2011 [==============================] - 0s 89us/step - loss: 0.3065 - acc: 0.6987\n",
      "Epoch 48/196\n",
      "2011/2011 [==============================] - 0s 107us/step - loss: 0.3063 - acc: 0.6987\n",
      "Epoch 49/196\n",
      "2011/2011 [==============================] - 0s 111us/step - loss: 0.3061 - acc: 0.6987\n",
      "Epoch 50/196\n",
      "2011/2011 [==============================] - 0s 112us/step - loss: 0.3059 - acc: 0.6987\n",
      "Epoch 51/196\n",
      "2011/2011 [==============================] - 0s 101us/step - loss: 0.3056 - acc: 0.6987\n",
      "Epoch 52/196\n",
      "2011/2011 [==============================] - 0s 120us/step - loss: 0.3055 - acc: 0.6987\n",
      "Epoch 53/196\n",
      "2011/2011 [==============================] - 0s 97us/step - loss: 0.3053 - acc: 0.6987\n",
      "Epoch 54/196\n",
      "2011/2011 [==============================] - 0s 96us/step - loss: 0.3051 - acc: 0.6987\n",
      "Epoch 55/196\n",
      "2011/2011 [==============================] - 0s 102us/step - loss: 0.3049 - acc: 0.6987\n",
      "Epoch 56/196\n",
      "2011/2011 [==============================] - 0s 104us/step - loss: 0.3048 - acc: 0.6987\n",
      "Epoch 57/196\n",
      "2011/2011 [==============================] - 0s 115us/step - loss: 0.3046 - acc: 0.6987\n",
      "Epoch 58/196\n",
      "2011/2011 [==============================] - 0s 104us/step - loss: 0.3045 - acc: 0.6987\n",
      "Epoch 59/196\n",
      "2011/2011 [==============================] - 0s 103us/step - loss: 0.3043 - acc: 0.6987\n",
      "Epoch 60/196\n",
      "2011/2011 [==============================] - 0s 118us/step - loss: 0.3042 - acc: 0.6987\n",
      "Epoch 61/196\n",
      "2011/2011 [==============================] - 0s 106us/step - loss: 0.3041 - acc: 0.6987\n",
      "Epoch 62/196\n",
      " 312/2011 [===>..........................] - ETA: 0s - loss: 0.2819 - acc: 0.7212"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6e4eb2760ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             classifier.fit(X_train, y_train, batch_size=bs, epochs=ep,\n\u001b[0;32m---> 11\u001b[0;31m                            class_weight='balanced', shuffle=False)\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0my_pred_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for clm in all_cols_try:\n",
    "    st = time()\n",
    "    X_train, X_test, y_train, y_test, X = xy_custom(df, y, 100, clm)\n",
    "    std_sclr = StandardScaler()\n",
    "    X_train = std_sclr.fit_transform(X_train)\n",
    "    X_test = std_sclr.fit_transform(X_test)\n",
    "    for bs in [6, 12, 24, 36, 48, 96]:\n",
    "        for ep in [48, 96, 192]:\n",
    "            classifier = _classifier()\n",
    "            classifier.fit(X_train, y_train, batch_size=bs, epochs=ep,\n",
    "                          class_weight='balanced', shuffle=False, verbose=2)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            y_pred = (y_pred > 0.5)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            results[(f'cols:{clm}, bs:{bs}, ep:{ep} (cm)')] = list(cm)\n",
    "            results[(f'cols:{clm}, bs:{bs}, ep:{ep} (accuracy)')\n",
    "                    ] = (f'{((cm[0][0]+cm[1][1])/cm.sum()*100).round(1)}%')\n",
    "        print(clm)\n",
    "    print(results)\n",
    "    print(time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test, X, y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T21:43:51.924844Z",
     "start_time": "2018-11-21T21:43:51.889969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y Shape: (2117,)\n",
      "X Shape: (2117, 1)\n",
      "X_train Shape: (2093, 1)\n",
      "X_test Shape: (24, 1)\n",
      "y_train Shape: (2093,)\n",
      "y_test Shape: (24,)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = get_Xy_train_test(\n",
    "#     df, .98, .99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data for easier computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T21:43:51.976192Z",
     "start_time": "2018-11-21T21:43:51.949736Z"
    }
   },
   "outputs": [],
   "source": [
    "# std_sclr = StandardScaler()\n",
    "# X_train = std_sclr.fit_transform(X_train)\n",
    "# X_test = std_sclr.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the scale of each value if interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T21:43:52.010782Z",
     "start_time": "2018-11-21T21:43:51.978571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.59603896])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# std_sclr.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I wanted to test different batch sizes to get the results on the real games. \n",
    "\n",
    "I started testing fitting classifier __For Loop__ with the following parameters:\n",
    "\n",
    "1. Batch Size:\n",
    "    1. 20\n",
    "    2. 16\n",
    "    3. 12\n",
    "    4. 8\n",
    "    5. 6\n",
    "2. Epochs:\n",
    "    1. 50\n",
    "    2. 100\n",
    "    3. 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T21:43:59.372972Z",
     "start_time": "2018-11-21T21:43:59.347547Z"
    }
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T21:46:47.686797Z",
     "start_time": "2018-11-21T21:44:00.191712Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2093/2093 [==============================] - 1s 380us/step - loss: 0.6933 - acc: 0.5055\n",
      "Epoch 2/50\n",
      "2093/2093 [==============================] - 0s 140us/step - loss: 0.6931 - acc: 0.5055\n",
      "Epoch 3/50\n",
      "2093/2093 [==============================] - 0s 134us/step - loss: 0.6917 - acc: 0.5093\n",
      "Epoch 4/50\n",
      "2093/2093 [==============================] - 0s 147us/step - loss: 0.6842 - acc: 0.6603\n",
      "Epoch 5/50\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.6635 - acc: 0.6971\n",
      "Epoch 6/50\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.6371 - acc: 0.6933\n",
      "Epoch 7/50\n",
      "2093/2093 [==============================] - 0s 133us/step - loss: 0.6160 - acc: 0.6933\n",
      "Epoch 8/50\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.6023 - acc: 0.6980\n",
      "Epoch 9/50\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.5940 - acc: 0.6985\n",
      "Epoch 10/50\n",
      "2093/2093 [==============================] - 0s 149us/step - loss: 0.5888 - acc: 0.6985\n",
      "Epoch 11/50\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5853 - acc: 0.6985\n",
      "Epoch 12/50\n",
      "2093/2093 [==============================] - 0s 176us/step - loss: 0.5827 - acc: 0.6976\n",
      "Epoch 13/50\n",
      "2093/2093 [==============================] - 0s 145us/step - loss: 0.5805 - acc: 0.6985\n",
      "Epoch 14/50\n",
      "2093/2093 [==============================] - 0s 161us/step - loss: 0.5785 - acc: 0.6990\n",
      "Epoch 15/50\n",
      "2093/2093 [==============================] - 0s 140us/step - loss: 0.5767 - acc: 0.6990\n",
      "Epoch 16/50\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5751 - acc: 0.6985\n",
      "Epoch 17/50\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5736 - acc: 0.6985\n",
      "Epoch 18/50\n",
      "2093/2093 [==============================] - 0s 173us/step - loss: 0.5722 - acc: 0.7000\n",
      "Epoch 19/50\n",
      "2093/2093 [==============================] - 0s 154us/step - loss: 0.5710 - acc: 0.7004\n",
      "Epoch 20/50\n",
      "2093/2093 [==============================] - 0s 166us/step - loss: 0.5699 - acc: 0.6990\n",
      "Epoch 21/50\n",
      "2093/2093 [==============================] - 0s 159us/step - loss: 0.5689 - acc: 0.6947\n",
      "Epoch 22/50\n",
      "2093/2093 [==============================] - 0s 162us/step - loss: 0.5680 - acc: 0.6961\n",
      "Epoch 23/50\n",
      "2093/2093 [==============================] - 0s 164us/step - loss: 0.5672 - acc: 0.6957\n",
      "Epoch 24/50\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5666 - acc: 0.6957\n",
      "Epoch 25/50\n",
      "2093/2093 [==============================] - 0s 172us/step - loss: 0.5660 - acc: 0.6966\n",
      "Epoch 26/50\n",
      "2093/2093 [==============================] - 0s 180us/step - loss: 0.5655 - acc: 0.6966\n",
      "Epoch 27/50\n",
      "2093/2093 [==============================] - 0s 163us/step - loss: 0.5651 - acc: 0.6971\n",
      "Epoch 28/50\n",
      "2093/2093 [==============================] - 0s 173us/step - loss: 0.5648 - acc: 0.6971\n",
      "Epoch 29/50\n",
      "2093/2093 [==============================] - 0s 167us/step - loss: 0.5645 - acc: 0.6971\n",
      "Epoch 30/50\n",
      "2093/2093 [==============================] - 0s 205us/step - loss: 0.5642 - acc: 0.6971\n",
      "Epoch 31/50\n",
      "2093/2093 [==============================] - 0s 139us/step - loss: 0.5640 - acc: 0.6971\n",
      "Epoch 32/50\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5639 - acc: 0.6971\n",
      "Epoch 33/50\n",
      "2093/2093 [==============================] - 0s 160us/step - loss: 0.5637 - acc: 0.6971\n",
      "Epoch 34/50\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5636 - acc: 0.6971\n",
      "Epoch 35/50\n",
      "2093/2093 [==============================] - 0s 173us/step - loss: 0.5635 - acc: 0.6971\n",
      "Epoch 36/50\n",
      "2093/2093 [==============================] - 0s 169us/step - loss: 0.5634 - acc: 0.6971\n",
      "Epoch 37/50\n",
      "2093/2093 [==============================] - 0s 170us/step - loss: 0.5633 - acc: 0.6971\n",
      "Epoch 38/50\n",
      "2093/2093 [==============================] - 0s 169us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 39/50\n",
      "2093/2093 [==============================] - 0s 136us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 40/50\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 41/50\n",
      "2093/2093 [==============================] - 0s 159us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 42/50\n",
      "2093/2093 [==============================] - 0s 139us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 43/50\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 44/50\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 45/50\n",
      "2093/2093 [==============================] - 0s 149us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 46/50\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 47/50\n",
      "2093/2093 [==============================] - 0s 140us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 48/50\n",
      "2093/2093 [==============================] - 0s 162us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 49/50\n",
      "2093/2093 [==============================] - 0s 160us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 50/50\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5626 - acc: 0.6971\n",
      "{'bs:20, ep:50 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:50 (accuracy)': '75.0%'}\n",
      "Epoch 1/100\n",
      "2093/2093 [==============================] - 1s 392us/step - loss: 0.6933 - acc: 0.4950\n",
      "Epoch 2/100\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.6931 - acc: 0.5055\n",
      "Epoch 3/100\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.6916 - acc: 0.5093\n",
      "Epoch 4/100\n",
      "2093/2093 [==============================] - 0s 145us/step - loss: 0.6840 - acc: 0.6593\n",
      "Epoch 5/100\n",
      "2093/2093 [==============================] - 0s 151us/step - loss: 0.6633 - acc: 0.6957\n",
      "Epoch 6/100\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.6367 - acc: 0.6952\n",
      "Epoch 7/100\n",
      "2093/2093 [==============================] - 0s 148us/step - loss: 0.6157 - acc: 0.6928\n",
      "Epoch 8/100\n",
      "2093/2093 [==============================] - 0s 148us/step - loss: 0.6022 - acc: 0.6971\n",
      "Epoch 9/100\n",
      "2093/2093 [==============================] - 0s 181us/step - loss: 0.5941 - acc: 0.6971\n",
      "Epoch 10/100\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5890 - acc: 0.6971\n",
      "Epoch 11/100\n",
      "2093/2093 [==============================] - 0s 177us/step - loss: 0.5855 - acc: 0.6971\n",
      "Epoch 12/100\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5829 - acc: 0.6966\n",
      "Epoch 13/100\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5807 - acc: 0.6980\n",
      "Epoch 14/100\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.5788 - acc: 0.6995\n",
      "Epoch 15/100\n",
      "2093/2093 [==============================] - 0s 181us/step - loss: 0.5771 - acc: 0.6990\n",
      "Epoch 16/100\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.5755 - acc: 0.6976\n",
      "Epoch 17/100\n",
      "2093/2093 [==============================] - 0s 170us/step - loss: 0.5741 - acc: 0.6947\n",
      "Epoch 18/100\n",
      "2093/2093 [==============================] - 0s 191us/step - loss: 0.5728 - acc: 0.6947\n",
      "Epoch 19/100\n",
      "2093/2093 [==============================] - 0s 193us/step - loss: 0.5716 - acc: 0.6957\n",
      "Epoch 20/100\n",
      "2093/2093 [==============================] - 0s 170us/step - loss: 0.5706 - acc: 0.6957\n",
      "Epoch 21/100\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5696 - acc: 0.6961\n",
      "Epoch 22/100\n",
      "2093/2093 [==============================] - 0s 163us/step - loss: 0.5688 - acc: 0.6957\n",
      "Epoch 23/100\n",
      "2093/2093 [==============================] - 0s 182us/step - loss: 0.5680 - acc: 0.6957\n",
      "Epoch 24/100\n",
      "2093/2093 [==============================] - 0s 189us/step - loss: 0.5673 - acc: 0.6966\n",
      "Epoch 25/100\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5668 - acc: 0.6966\n",
      "Epoch 26/100\n",
      "2093/2093 [==============================] - 0s 181us/step - loss: 0.5663 - acc: 0.6966\n",
      "Epoch 27/100\n",
      "2093/2093 [==============================] - 0s 215us/step - loss: 0.5658 - acc: 0.6966\n",
      "Epoch 28/100\n",
      "2093/2093 [==============================] - 0s 166us/step - loss: 0.5654 - acc: 0.6971\n",
      "Epoch 29/100\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5651 - acc: 0.6971\n",
      "Epoch 30/100\n",
      "2093/2093 [==============================] - 0s 188us/step - loss: 0.5648 - acc: 0.6971\n",
      "Epoch 31/100\n",
      "2093/2093 [==============================] - 0s 179us/step - loss: 0.5646 - acc: 0.6971\n",
      "Epoch 32/100\n",
      "2093/2093 [==============================] - 0s 160us/step - loss: 0.5643 - acc: 0.6971\n",
      "Epoch 33/100\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5642 - acc: 0.6971\n",
      "Epoch 34/100\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5640 - acc: 0.6971\n",
      "Epoch 35/100\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5638 - acc: 0.6971\n",
      "Epoch 36/100\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5637 - acc: 0.6971\n",
      "Epoch 37/100\n",
      "2093/2093 [==============================] - 0s 147us/step - loss: 0.5636 - acc: 0.6971\n",
      "Epoch 38/100\n",
      "2093/2093 [==============================] - 0s 149us/step - loss: 0.5635 - acc: 0.6971\n",
      "Epoch 39/100\n",
      "2093/2093 [==============================] - 0s 177us/step - loss: 0.5634 - acc: 0.6971\n",
      "Epoch 40/100\n",
      "2093/2093 [==============================] - 0s 160us/step - loss: 0.5633 - acc: 0.6971\n",
      "Epoch 41/100\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.5633 - acc: 0.6971\n",
      "Epoch 42/100\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 43/100\n",
      "2093/2093 [==============================] - 0s 162us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 44/100\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 45/100\n",
      "2093/2093 [==============================] - 0s 149us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 46/100\n",
      "2093/2093 [==============================] - 0s 125us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 47/100\n",
      "2093/2093 [==============================] - 0s 136us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 48/100\n",
      "2093/2093 [==============================] - 0s 129us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 49/100\n",
      "2093/2093 [==============================] - 0s 140us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 50/100\n",
      "2093/2093 [==============================] - 0s 137us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 51/100\n",
      "2093/2093 [==============================] - 0s 129us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 52/100\n",
      "2093/2093 [==============================] - 0s 137us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 53/100\n",
      "2093/2093 [==============================] - 0s 199us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 54/100\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 55/100\n",
      "2093/2093 [==============================] - 0s 192us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 56/100\n",
      "2093/2093 [==============================] - 0s 166us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 57/100\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 58/100\n",
      "2093/2093 [==============================] - 0s 176us/step - loss: 0.5626 - acc: 0.6971\n",
      "Epoch 59/100\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.5626 - acc: 0.6971\n",
      "Epoch 60/100\n",
      "2093/2093 [==============================] - 0s 173us/step - loss: 0.5626 - acc: 0.6971\n",
      "Epoch 61/100\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5626 - acc: 0.6971\n",
      "Epoch 62/100\n",
      "2093/2093 [==============================] - 0s 162us/step - loss: 0.5625 - acc: 0.6971\n",
      "Epoch 63/100\n",
      "2093/2093 [==============================] - 0s 137us/step - loss: 0.5625 - acc: 0.6971\n",
      "Epoch 64/100\n",
      "2093/2093 [==============================] - 0s 184us/step - loss: 0.5625 - acc: 0.6971\n",
      "Epoch 65/100\n",
      "2093/2093 [==============================] - 0s 159us/step - loss: 0.5625 - acc: 0.6971\n",
      "Epoch 66/100\n",
      "2093/2093 [==============================] - 0s 175us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 67/100\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 68/100\n",
      "2093/2093 [==============================] - 0s 166us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 69/100\n",
      "2093/2093 [==============================] - 0s 143us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 70/100\n",
      "2093/2093 [==============================] - 0s 174us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 71/100\n",
      "2093/2093 [==============================] - 0s 154us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 72/100\n",
      "2093/2093 [==============================] - 0s 168us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 73/100\n",
      "2093/2093 [==============================] - 0s 167us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 74/100\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 75/100\n",
      "2093/2093 [==============================] - 0s 169us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 76/100\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 77/100\n",
      "2093/2093 [==============================] - 0s 178us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 78/100\n",
      "2093/2093 [==============================] - 0s 161us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 79/100\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5621 - acc: 0.6971\n",
      "Epoch 80/100\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5621 - acc: 0.6971\n",
      "Epoch 81/100\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5621 - acc: 0.6971\n",
      "Epoch 82/100\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5621 - acc: 0.6971\n",
      "Epoch 83/100\n",
      "2093/2093 [==============================] - 0s 156us/step - loss: 0.5620 - acc: 0.6971\n",
      "Epoch 84/100\n",
      "2093/2093 [==============================] - 0s 169us/step - loss: 0.5620 - acc: 0.6971\n",
      "Epoch 85/100\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5620 - acc: 0.6971\n",
      "Epoch 86/100\n",
      "2093/2093 [==============================] - 0s 173us/step - loss: 0.5620 - acc: 0.6971\n",
      "Epoch 87/100\n",
      "2093/2093 [==============================] - 0s 173us/step - loss: 0.5619 - acc: 0.6971\n",
      "Epoch 88/100\n",
      "2093/2093 [==============================] - 0s 159us/step - loss: 0.5619 - acc: 0.6971\n",
      "Epoch 89/100\n",
      "2093/2093 [==============================] - 0s 161us/step - loss: 0.5619 - acc: 0.6971\n",
      "Epoch 90/100\n",
      "2093/2093 [==============================] - 0s 173us/step - loss: 0.5619 - acc: 0.6971\n",
      "Epoch 91/100\n",
      "2093/2093 [==============================] - 0s 170us/step - loss: 0.5618 - acc: 0.6971\n",
      "Epoch 92/100\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.5618 - acc: 0.6971\n",
      "Epoch 93/100\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.5618 - acc: 0.6971\n",
      "Epoch 94/100\n",
      "2093/2093 [==============================] - 0s 148us/step - loss: 0.5617 - acc: 0.6971\n",
      "Epoch 95/100\n",
      "2093/2093 [==============================] - 0s 168us/step - loss: 0.5617 - acc: 0.6971\n",
      "Epoch 96/100\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5617 - acc: 0.6971\n",
      "Epoch 97/100\n",
      "2093/2093 [==============================] - 0s 156us/step - loss: 0.5617 - acc: 0.6971\n",
      "Epoch 98/100\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.5616 - acc: 0.6971\n",
      "Epoch 99/100\n",
      "2093/2093 [==============================] - 0s 177us/step - loss: 0.5616 - acc: 0.6971\n",
      "Epoch 100/100\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.5616 - acc: 0.6971\n",
      "{'bs:20, ep:50 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:50 (accuracy)': '75.0%', 'bs:20, ep:100 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:100 (accuracy)': '75.0%'}\n",
      "Epoch 1/200\n",
      "2093/2093 [==============================] - 1s 430us/step - loss: 0.6933 - acc: 0.4806\n",
      "Epoch 2/200\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.6931 - acc: 0.5055\n",
      "Epoch 3/200\n",
      "2093/2093 [==============================] - 0s 156us/step - loss: 0.6917 - acc: 0.5117\n",
      "Epoch 4/200\n",
      "2093/2093 [==============================] - 0s 167us/step - loss: 0.6840 - acc: 0.6636\n",
      "Epoch 5/200\n",
      "2093/2093 [==============================] - 0s 148us/step - loss: 0.6627 - acc: 0.6957\n",
      "Epoch 6/200\n",
      "2093/2093 [==============================] - 0s 147us/step - loss: 0.6358 - acc: 0.6918\n",
      "Epoch 7/200\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.6150 - acc: 0.6933\n",
      "Epoch 8/200\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.6019 - acc: 0.6971\n",
      "Epoch 9/200\n",
      "2093/2093 [==============================] - 0s 172us/step - loss: 0.5939 - acc: 0.6971\n",
      "Epoch 10/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2093/2093 [==============================] - 0s 134us/step - loss: 0.5889 - acc: 0.6980\n",
      "Epoch 11/200\n",
      "2093/2093 [==============================] - 0s 148us/step - loss: 0.5855 - acc: 0.6980\n",
      "Epoch 12/200\n",
      "2093/2093 [==============================] - 0s 134us/step - loss: 0.5829 - acc: 0.6980\n",
      "Epoch 13/200\n",
      "2093/2093 [==============================] - 0s 140us/step - loss: 0.5807 - acc: 0.6980\n",
      "Epoch 14/200\n",
      "2093/2093 [==============================] - 0s 135us/step - loss: 0.5788 - acc: 0.6980\n",
      "Epoch 15/200\n",
      "2093/2093 [==============================] - 0s 133us/step - loss: 0.5771 - acc: 0.6980\n",
      "Epoch 16/200\n",
      "2093/2093 [==============================] - 0s 135us/step - loss: 0.5755 - acc: 0.6980\n",
      "Epoch 17/200\n",
      "2093/2093 [==============================] - 0s 138us/step - loss: 0.5741 - acc: 0.6980\n",
      "Epoch 18/200\n",
      "2093/2093 [==============================] - 0s 140us/step - loss: 0.5728 - acc: 0.7000\n",
      "Epoch 19/200\n",
      "2093/2093 [==============================] - 0s 133us/step - loss: 0.5716 - acc: 0.7004\n",
      "Epoch 20/200\n",
      "2093/2093 [==============================] - 0s 132us/step - loss: 0.5705 - acc: 0.6995\n",
      "Epoch 21/200\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5696 - acc: 0.6980\n",
      "Epoch 22/200\n",
      "2093/2093 [==============================] - 0s 154us/step - loss: 0.5687 - acc: 0.6947\n",
      "Epoch 23/200\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5680 - acc: 0.6957\n",
      "Epoch 24/200\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5673 - acc: 0.6957\n",
      "Epoch 25/200\n",
      "2093/2093 [==============================] - 0s 139us/step - loss: 0.5668 - acc: 0.6957\n",
      "Epoch 26/200\n",
      "2093/2093 [==============================] - 0s 159us/step - loss: 0.5663 - acc: 0.6957\n",
      "Epoch 27/200\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5658 - acc: 0.6966\n",
      "Epoch 28/200\n",
      "2093/2093 [==============================] - 0s 162us/step - loss: 0.5655 - acc: 0.6966\n",
      "Epoch 29/200\n",
      "2093/2093 [==============================] - 0s 143us/step - loss: 0.5651 - acc: 0.6971\n",
      "Epoch 30/200\n",
      "2093/2093 [==============================] - 0s 168us/step - loss: 0.5649 - acc: 0.6971\n",
      "Epoch 31/200\n",
      "2093/2093 [==============================] - 0s 167us/step - loss: 0.5646 - acc: 0.6971\n",
      "Epoch 32/200\n",
      "2093/2093 [==============================] - 0s 145us/step - loss: 0.5644 - acc: 0.6971\n",
      "Epoch 33/200\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.5642 - acc: 0.6971\n",
      "Epoch 34/200\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5641 - acc: 0.6971\n",
      "Epoch 35/200\n",
      "2093/2093 [==============================] - 0s 143us/step - loss: 0.5639 - acc: 0.6971\n",
      "Epoch 36/200\n",
      "2093/2093 [==============================] - 0s 156us/step - loss: 0.5638 - acc: 0.6971\n",
      "Epoch 37/200\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5637 - acc: 0.6971\n",
      "Epoch 38/200\n",
      "2093/2093 [==============================] - 0s 127us/step - loss: 0.5636 - acc: 0.6971\n",
      "Epoch 39/200\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5635 - acc: 0.6971\n",
      "Epoch 40/200\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5635 - acc: 0.6971\n",
      "Epoch 41/200\n",
      "2093/2093 [==============================] - 0s 156us/step - loss: 0.5634 - acc: 0.6971\n",
      "Epoch 42/200\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.5633 - acc: 0.6971\n",
      "Epoch 43/200\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5633 - acc: 0.6971\n",
      "Epoch 44/200\n",
      "2093/2093 [==============================] - 0s 147us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 45/200\n",
      "2093/2093 [==============================] - 0s 151us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 46/200\n",
      "2093/2093 [==============================] - 0s 136us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 47/200\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 48/200\n",
      "2093/2093 [==============================] - 0s 147us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 49/200\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 50/200\n",
      "2093/2093 [==============================] - 0s 147us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 51/200\n",
      "2093/2093 [==============================] - 0s 137us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 52/200\n",
      "2093/2093 [==============================] - 0s 130us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 53/200\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 54/200\n",
      "2093/2093 [==============================] - 0s 203us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 55/200\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 56/200\n",
      "2093/2093 [==============================] - 0s 166us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 57/200\n",
      "2093/2093 [==============================] - 0s 164us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 58/200\n",
      "2093/2093 [==============================] - 0s 149us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 59/200\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 60/200\n",
      "2093/2093 [==============================] - 0s 170us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 61/200\n",
      "2093/2093 [==============================] - 0s 160us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 62/200\n",
      "2093/2093 [==============================] - 0s 177us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 63/200\n",
      "2093/2093 [==============================] - 0s 161us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 64/200\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5626 - acc: 0.6971\n",
      "Epoch 65/200\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.5626 - acc: 0.6971\n",
      "Epoch 66/200\n",
      "2093/2093 [==============================] - 0s 140us/step - loss: 0.5626 - acc: 0.6971\n",
      "Epoch 67/200\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5626 - acc: 0.6971\n",
      "Epoch 68/200\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5625 - acc: 0.6971\n",
      "Epoch 69/200\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.5625 - acc: 0.6971\n",
      "Epoch 70/200\n",
      "2093/2093 [==============================] - 0s 161us/step - loss: 0.5625 - acc: 0.6971\n",
      "Epoch 71/200\n",
      "2093/2093 [==============================] - 0s 179us/step - loss: 0.5625 - acc: 0.6971\n",
      "Epoch 72/200\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 73/200\n",
      "2093/2093 [==============================] - 0s 145us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 74/200\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 75/200\n",
      "2093/2093 [==============================] - 0s 183us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 76/200\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 77/200\n",
      "2093/2093 [==============================] - 0s 186us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 78/200\n",
      "2093/2093 [==============================] - 0s 162us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 79/200\n",
      "2093/2093 [==============================] - 0s 182us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 80/200\n",
      "2093/2093 [==============================] - 0s 164us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 81/200\n",
      "2093/2093 [==============================] - 0s 183us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 82/200\n",
      "2093/2093 [==============================] - 0s 149us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 83/200\n",
      "2093/2093 [==============================] - 0s 187us/step - loss: 0.5621 - acc: 0.6971\n",
      "Epoch 84/200\n",
      "2093/2093 [==============================] - 0s 202us/step - loss: 0.5621 - acc: 0.6971\n",
      "Epoch 85/200\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.5621 - acc: 0.6971\n",
      "Epoch 86/200\n",
      "2093/2093 [==============================] - 0s 194us/step - loss: 0.5620 - acc: 0.6971\n",
      "Epoch 87/200\n",
      "2093/2093 [==============================] - 0s 169us/step - loss: 0.5620 - acc: 0.6971\n",
      "Epoch 88/200\n",
      "2093/2093 [==============================] - 0s 199us/step - loss: 0.5620 - acc: 0.6971\n",
      "Epoch 89/200\n",
      "2093/2093 [==============================] - 0s 212us/step - loss: 0.5620 - acc: 0.6971\n",
      "Epoch 90/200\n",
      "2093/2093 [==============================] - 0s 201us/step - loss: 0.5619 - acc: 0.6971\n",
      "Epoch 91/200\n",
      "2093/2093 [==============================] - 0s 197us/step - loss: 0.5619 - acc: 0.6971\n",
      "Epoch 92/200\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5619 - acc: 0.6971\n",
      "Epoch 93/200\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.5618 - acc: 0.6971\n",
      "Epoch 94/200\n",
      "2093/2093 [==============================] - 0s 148us/step - loss: 0.5618 - acc: 0.6971\n",
      "Epoch 95/200\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.5618 - acc: 0.6971\n",
      "Epoch 96/200\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5617 - acc: 0.6971\n",
      "Epoch 97/200\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5617 - acc: 0.6971\n",
      "Epoch 98/200\n",
      "2093/2093 [==============================] - 0s 139us/step - loss: 0.5617 - acc: 0.6971\n",
      "Epoch 99/200\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5616 - acc: 0.6971\n",
      "Epoch 100/200\n",
      "2093/2093 [==============================] - 0s 160us/step - loss: 0.5616 - acc: 0.6971\n",
      "Epoch 101/200\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5616 - acc: 0.6971\n",
      "Epoch 102/200\n",
      "2093/2093 [==============================] - 0s 163us/step - loss: 0.5615 - acc: 0.6971\n",
      "Epoch 103/200\n",
      "2093/2093 [==============================] - 0s 168us/step - loss: 0.5615 - acc: 0.6961\n",
      "Epoch 104/200\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5615 - acc: 0.6961\n",
      "Epoch 105/200\n",
      "2093/2093 [==============================] - 0s 137us/step - loss: 0.5614 - acc: 0.6961\n",
      "Epoch 106/200\n",
      "2093/2093 [==============================] - 0s 172us/step - loss: 0.5614 - acc: 0.6961\n",
      "Epoch 107/200\n",
      "2093/2093 [==============================] - 0s 138us/step - loss: 0.5614 - acc: 0.6961\n",
      "Epoch 108/200\n",
      "2093/2093 [==============================] - 0s 151us/step - loss: 0.5613 - acc: 0.6957\n",
      "Epoch 109/200\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5613 - acc: 0.6957\n",
      "Epoch 110/200\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5613 - acc: 0.6952\n",
      "Epoch 111/200\n",
      "2093/2093 [==============================] - 0s 136us/step - loss: 0.5612 - acc: 0.6952\n",
      "Epoch 112/200\n",
      "2093/2093 [==============================] - 0s 161us/step - loss: 0.5612 - acc: 0.6952\n",
      "Epoch 113/200\n",
      "2093/2093 [==============================] - 0s 160us/step - loss: 0.5612 - acc: 0.6966\n",
      "Epoch 114/200\n",
      "2093/2093 [==============================] - 0s 138us/step - loss: 0.5611 - acc: 0.6966\n",
      "Epoch 115/200\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5611 - acc: 0.6966\n",
      "Epoch 116/200\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5611 - acc: 0.6966\n",
      "Epoch 117/200\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5610 - acc: 0.6971\n",
      "Epoch 118/200\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.5610 - acc: 0.6971\n",
      "Epoch 119/200\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.5610 - acc: 0.6976\n",
      "Epoch 120/200\n",
      "2093/2093 [==============================] - 0s 143us/step - loss: 0.5609 - acc: 0.6971\n",
      "Epoch 121/200\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.5609 - acc: 0.6961\n",
      "Epoch 122/200\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5608 - acc: 0.6961\n",
      "Epoch 123/200\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.5608 - acc: 0.6966\n",
      "Epoch 124/200\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5608 - acc: 0.6971\n",
      "Epoch 125/200\n",
      "2093/2093 [==============================] - 0s 174us/step - loss: 0.5607 - acc: 0.6966\n",
      "Epoch 126/200\n",
      "2093/2093 [==============================] - 0s 154us/step - loss: 0.5607 - acc: 0.6966\n",
      "Epoch 127/200\n",
      "2093/2093 [==============================] - 0s 176us/step - loss: 0.5607 - acc: 0.6966\n",
      "Epoch 128/200\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5606 - acc: 0.6971\n",
      "Epoch 129/200\n",
      "2093/2093 [==============================] - 0s 145us/step - loss: 0.5606 - acc: 0.6976\n",
      "Epoch 130/200\n",
      "2093/2093 [==============================] - 0s 174us/step - loss: 0.5605 - acc: 0.6976\n",
      "Epoch 131/200\n",
      "2093/2093 [==============================] - 0s 148us/step - loss: 0.5605 - acc: 0.6976\n",
      "Epoch 132/200\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.5605 - acc: 0.6976\n",
      "Epoch 133/200\n",
      "2093/2093 [==============================] - 0s 166us/step - loss: 0.5604 - acc: 0.6976\n",
      "Epoch 134/200\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5604 - acc: 0.6976\n",
      "Epoch 135/200\n",
      "2093/2093 [==============================] - 0s 153us/step - loss: 0.5603 - acc: 0.6976\n",
      "Epoch 136/200\n",
      "2093/2093 [==============================] - 0s 139us/step - loss: 0.5603 - acc: 0.6990\n",
      "Epoch 137/200\n",
      "2093/2093 [==============================] - 0s 151us/step - loss: 0.5603 - acc: 0.6990\n",
      "Epoch 138/200\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5602 - acc: 0.6990\n",
      "Epoch 139/200\n",
      "2093/2093 [==============================] - 0s 159us/step - loss: 0.5602 - acc: 0.7000\n",
      "Epoch 140/200\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.5601 - acc: 0.7000\n",
      "Epoch 141/200\n",
      "2093/2093 [==============================] - 0s 139us/step - loss: 0.5601 - acc: 0.7000\n",
      "Epoch 142/200\n",
      "2093/2093 [==============================] - 0s 137us/step - loss: 0.5601 - acc: 0.7000\n",
      "Epoch 143/200\n",
      "2093/2093 [==============================] - 0s 172us/step - loss: 0.5600 - acc: 0.7004\n",
      "Epoch 144/200\n",
      "2093/2093 [==============================] - 0s 128us/step - loss: 0.5600 - acc: 0.7004\n",
      "Epoch 145/200\n",
      "2093/2093 [==============================] - 0s 148us/step - loss: 0.5599 - acc: 0.7004\n",
      "Epoch 146/200\n",
      "2093/2093 [==============================] - 0s 176us/step - loss: 0.5599 - acc: 0.7004\n",
      "Epoch 147/200\n",
      "2093/2093 [==============================] - 0s 149us/step - loss: 0.5598 - acc: 0.7004\n",
      "Epoch 148/200\n",
      "2093/2093 [==============================] - 0s 147us/step - loss: 0.5598 - acc: 0.7004\n",
      "Epoch 149/200\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5598 - acc: 0.7004\n",
      "Epoch 150/200\n",
      "2093/2093 [==============================] - 0s 207us/step - loss: 0.5597 - acc: 0.6995\n",
      "Epoch 151/200\n",
      "2093/2093 [==============================] - 0s 168us/step - loss: 0.5597 - acc: 0.7000\n",
      "Epoch 152/200\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5596 - acc: 0.7000\n",
      "Epoch 153/200\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5596 - acc: 0.7004\n",
      "Epoch 154/200\n",
      "2093/2093 [==============================] - 0s 149us/step - loss: 0.5595 - acc: 0.7004\n",
      "Epoch 155/200\n",
      "2093/2093 [==============================] - 0s 178us/step - loss: 0.5595 - acc: 0.7000\n",
      "Epoch 156/200\n",
      "2093/2093 [==============================] - 0s 145us/step - loss: 0.5594 - acc: 0.6985\n",
      "Epoch 157/200\n",
      "2093/2093 [==============================] - 0s 174us/step - loss: 0.5594 - acc: 0.6980\n",
      "Epoch 158/200\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.5594 - acc: 0.6976\n",
      "Epoch 159/200\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5593 - acc: 0.6976\n",
      "Epoch 160/200\n",
      "2093/2093 [==============================] - 0s 146us/step - loss: 0.5593 - acc: 0.6980\n",
      "Epoch 161/200\n",
      "2093/2093 [==============================] - 0s 182us/step - loss: 0.5592 - acc: 0.6985\n",
      "Epoch 162/200\n",
      "2093/2093 [==============================] - 0s 147us/step - loss: 0.5592 - acc: 0.6976\n",
      "Epoch 163/200\n",
      "2093/2093 [==============================] - 0s 174us/step - loss: 0.5591 - acc: 0.6976\n",
      "Epoch 164/200\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5591 - acc: 0.6976\n",
      "Epoch 165/200\n",
      "2093/2093 [==============================] - 0s 168us/step - loss: 0.5590 - acc: 0.6976\n",
      "Epoch 166/200\n",
      "2093/2093 [==============================] - 0s 152us/step - loss: 0.5590 - acc: 0.6976\n",
      "Epoch 167/200\n",
      "2093/2093 [==============================] - 0s 169us/step - loss: 0.5589 - acc: 0.6971\n",
      "Epoch 168/200\n",
      "2093/2093 [==============================] - 0s 151us/step - loss: 0.5589 - acc: 0.6966\n",
      "Epoch 169/200\n",
      "2093/2093 [==============================] - 0s 165us/step - loss: 0.5589 - acc: 0.6966\n",
      "Epoch 170/200\n",
      "2093/2093 [==============================] - 0s 158us/step - loss: 0.5588 - acc: 0.6957\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2093/2093 [==============================] - 0s 172us/step - loss: 0.5588 - acc: 0.6957\n",
      "Epoch 172/200\n",
      "2093/2093 [==============================] - 0s 141us/step - loss: 0.5587 - acc: 0.6952\n",
      "Epoch 173/200\n",
      "2093/2093 [==============================] - 0s 166us/step - loss: 0.5587 - acc: 0.6952\n",
      "Epoch 174/200\n",
      "2093/2093 [==============================] - 0s 136us/step - loss: 0.5586 - acc: 0.6952\n",
      "Epoch 175/200\n",
      "2093/2093 [==============================] - 0s 163us/step - loss: 0.5586 - acc: 0.6952\n",
      "Epoch 176/200\n",
      "2093/2093 [==============================] - 0s 136us/step - loss: 0.5585 - acc: 0.6952\n",
      "Epoch 177/200\n",
      "2093/2093 [==============================] - 0s 168us/step - loss: 0.5585 - acc: 0.6952\n",
      "Epoch 178/200\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5584 - acc: 0.6947\n",
      "Epoch 179/200\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5584 - acc: 0.6942\n",
      "Epoch 180/200\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5583 - acc: 0.6942\n",
      "Epoch 181/200\n",
      "2093/2093 [==============================] - 0s 155us/step - loss: 0.5583 - acc: 0.6937\n",
      "Epoch 182/200\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5583 - acc: 0.6937\n",
      "Epoch 183/200\n",
      "2093/2093 [==============================] - 0s 163us/step - loss: 0.5582 - acc: 0.6942\n",
      "Epoch 184/200\n",
      "2093/2093 [==============================] - 0s 135us/step - loss: 0.5582 - acc: 0.6947\n",
      "Epoch 185/200\n",
      "2093/2093 [==============================] - 0s 156us/step - loss: 0.5581 - acc: 0.6947\n",
      "Epoch 186/200\n",
      "2093/2093 [==============================] - 0s 144us/step - loss: 0.5581 - acc: 0.6947\n",
      "Epoch 187/200\n",
      "2093/2093 [==============================] - 0s 167us/step - loss: 0.5580 - acc: 0.6942\n",
      "Epoch 188/200\n",
      "2093/2093 [==============================] - 0s 154us/step - loss: 0.5580 - acc: 0.6957\n",
      "Epoch 189/200\n",
      "2093/2093 [==============================] - 0s 197us/step - loss: 0.5579 - acc: 0.6961\n",
      "Epoch 190/200\n",
      "2093/2093 [==============================] - 0s 129us/step - loss: 0.5579 - acc: 0.6952\n",
      "Epoch 191/200\n",
      "2093/2093 [==============================] - 0s 171us/step - loss: 0.5578 - acc: 0.6947\n",
      "Epoch 192/200\n",
      "2093/2093 [==============================] - 0s 166us/step - loss: 0.5578 - acc: 0.6947\n",
      "Epoch 193/200\n",
      "2093/2093 [==============================] - 0s 157us/step - loss: 0.5578 - acc: 0.6947\n",
      "Epoch 194/200\n",
      "2093/2093 [==============================] - 0s 184us/step - loss: 0.5577 - acc: 0.6942\n",
      "Epoch 195/200\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5577 - acc: 0.6942\n",
      "Epoch 196/200\n",
      "2093/2093 [==============================] - 0s 150us/step - loss: 0.5576 - acc: 0.6942\n",
      "Epoch 197/200\n",
      "2093/2093 [==============================] - 0s 142us/step - loss: 0.5576 - acc: 0.6942\n",
      "Epoch 198/200\n",
      "2093/2093 [==============================] - 0s 159us/step - loss: 0.5575 - acc: 0.6942\n",
      "Epoch 199/200\n",
      "2093/2093 [==============================] - 0s 134us/step - loss: 0.5575 - acc: 0.6937\n",
      "Epoch 200/200\n",
      "2093/2093 [==============================] - 0s 143us/step - loss: 0.5574 - acc: 0.6928\n",
      "{'bs:20, ep:50 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:50 (accuracy)': '75.0%', 'bs:20, ep:100 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:100 (accuracy)': '75.0%', 'bs:20, ep:200 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:200 (accuracy)': '75.0%'}\n",
      "Epoch 1/50\n",
      "2093/2093 [==============================] - 1s 455us/step - loss: 0.6933 - acc: 0.4921\n",
      "Epoch 2/50\n",
      "2093/2093 [==============================] - 0s 207us/step - loss: 0.6929 - acc: 0.5055\n",
      "Epoch 3/50\n",
      "2093/2093 [==============================] - 0s 175us/step - loss: 0.6889 - acc: 0.6044\n",
      "Epoch 4/50\n",
      "2093/2093 [==============================] - 0s 189us/step - loss: 0.6699 - acc: 0.6942\n",
      "Epoch 5/50\n",
      "2093/2093 [==============================] - 0s 179us/step - loss: 0.6383 - acc: 0.6966\n",
      "Epoch 6/50\n",
      "2093/2093 [==============================] - 0s 185us/step - loss: 0.6137 - acc: 0.6957\n",
      "Epoch 7/50\n",
      "2093/2093 [==============================] - 0s 202us/step - loss: 0.5994 - acc: 0.6985\n",
      "Epoch 8/50\n",
      "2093/2093 [==============================] - 0s 191us/step - loss: 0.5914 - acc: 0.6980\n",
      "Epoch 9/50\n",
      "2093/2093 [==============================] - 0s 217us/step - loss: 0.5865 - acc: 0.6980\n",
      "Epoch 10/50\n",
      "2093/2093 [==============================] - 0s 216us/step - loss: 0.5832 - acc: 0.6980\n",
      "Epoch 11/50\n",
      "2093/2093 [==============================] - 0s 212us/step - loss: 0.5805 - acc: 0.6990\n",
      "Epoch 12/50\n",
      "2093/2093 [==============================] - 0s 207us/step - loss: 0.5783 - acc: 0.6995\n",
      "Epoch 13/50\n",
      "2093/2093 [==============================] - 0s 210us/step - loss: 0.5763 - acc: 0.6995\n",
      "Epoch 14/50\n",
      "2093/2093 [==============================] - 0s 188us/step - loss: 0.5744 - acc: 0.6995\n",
      "Epoch 15/50\n",
      "2093/2093 [==============================] - 0s 179us/step - loss: 0.5728 - acc: 0.7004\n",
      "Epoch 16/50\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5713 - acc: 0.7004\n",
      "Epoch 17/50\n",
      "2093/2093 [==============================] - 0s 219us/step - loss: 0.5700 - acc: 0.6985\n",
      "Epoch 18/50\n",
      "2093/2093 [==============================] - 0s 226us/step - loss: 0.5689 - acc: 0.6947\n",
      "Epoch 19/50\n",
      "2093/2093 [==============================] - 0s 209us/step - loss: 0.5680 - acc: 0.6961\n",
      "Epoch 20/50\n",
      "2093/2093 [==============================] - 0s 203us/step - loss: 0.5672 - acc: 0.6966\n",
      "Epoch 21/50\n",
      "2093/2093 [==============================] - 0s 221us/step - loss: 0.5665 - acc: 0.6961\n",
      "Epoch 22/50\n",
      "2093/2093 [==============================] - 0s 183us/step - loss: 0.5659 - acc: 0.6961\n",
      "Epoch 23/50\n",
      "2093/2093 [==============================] - 0s 205us/step - loss: 0.5655 - acc: 0.6957\n",
      "Epoch 24/50\n",
      "2093/2093 [==============================] - 0s 201us/step - loss: 0.5651 - acc: 0.6966\n",
      "Epoch 25/50\n",
      "2093/2093 [==============================] - 0s 201us/step - loss: 0.5648 - acc: 0.6966\n",
      "Epoch 26/50\n",
      "2093/2093 [==============================] - 0s 204us/step - loss: 0.5645 - acc: 0.6966\n",
      "Epoch 27/50\n",
      "2093/2093 [==============================] - 0s 203us/step - loss: 0.5643 - acc: 0.6966\n",
      "Epoch 28/50\n",
      "2093/2093 [==============================] - 0s 179us/step - loss: 0.5641 - acc: 0.6971\n",
      "Epoch 29/50\n",
      "2093/2093 [==============================] - 0s 208us/step - loss: 0.5639 - acc: 0.6971\n",
      "Epoch 30/50\n",
      "2093/2093 [==============================] - 0s 205us/step - loss: 0.5638 - acc: 0.6971\n",
      "Epoch 31/50\n",
      "2093/2093 [==============================] - 0s 222us/step - loss: 0.5637 - acc: 0.6971\n",
      "Epoch 32/50\n",
      "2093/2093 [==============================] - 0s 192us/step - loss: 0.5636 - acc: 0.6971\n",
      "Epoch 33/50\n",
      "2093/2093 [==============================] - 0s 218us/step - loss: 0.5635 - acc: 0.6971\n",
      "Epoch 34/50\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5634 - acc: 0.6971\n",
      "Epoch 35/50\n",
      "2093/2093 [==============================] - 0s 218us/step - loss: 0.5633 - acc: 0.6971\n",
      "Epoch 36/50\n",
      "2093/2093 [==============================] - 0s 195us/step - loss: 0.5633 - acc: 0.6971\n",
      "Epoch 37/50\n",
      "2093/2093 [==============================] - 0s 220us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 38/50\n",
      "2093/2093 [==============================] - 0s 205us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 39/50\n",
      "2093/2093 [==============================] - 0s 211us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 40/50\n",
      "2093/2093 [==============================] - 0s 215us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 41/50\n",
      "2093/2093 [==============================] - 0s 211us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 42/50\n",
      "2093/2093 [==============================] - 0s 201us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 43/50\n",
      "2093/2093 [==============================] - 0s 207us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 44/50\n",
      "2093/2093 [==============================] - 0s 215us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 45/50\n",
      "2093/2093 [==============================] - 0s 217us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 46/50\n",
      "2093/2093 [==============================] - 0s 226us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 47/50\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 48/50\n",
      "2093/2093 [==============================] - 0s 210us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 49/50\n",
      "2093/2093 [==============================] - 0s 180us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 50/50\n",
      "2093/2093 [==============================] - 0s 170us/step - loss: 0.5628 - acc: 0.6957\n",
      "{'bs:20, ep:50 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:50 (accuracy)': '75.0%', 'bs:20, ep:100 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:100 (accuracy)': '75.0%', 'bs:20, ep:200 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:20, ep:200 (accuracy)': '75.0%', 'bs:16, ep:50 (cm)': array([[ 8,  2],\n",
      "       [ 4, 10]]), 'bs:16, ep:50 (accuracy)': '75.0%'}\n",
      "Epoch 1/100\n",
      "2093/2093 [==============================] - 1s 488us/step - loss: 0.6933 - acc: 0.4998\n",
      "Epoch 2/100\n",
      "2093/2093 [==============================] - 0s 172us/step - loss: 0.6928 - acc: 0.5055\n",
      "Epoch 3/100\n",
      "2093/2093 [==============================] - 0s 184us/step - loss: 0.6888 - acc: 0.6106\n",
      "Epoch 4/100\n",
      "2093/2093 [==============================] - 0s 159us/step - loss: 0.6700 - acc: 0.6971\n",
      "Epoch 5/100\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.6391 - acc: 0.6918\n",
      "Epoch 6/100\n",
      "2093/2093 [==============================] - 0s 201us/step - loss: 0.6145 - acc: 0.6961\n",
      "Epoch 7/100\n",
      "2093/2093 [==============================] - 0s 176us/step - loss: 0.5998 - acc: 0.6976\n",
      "Epoch 8/100\n",
      "2093/2093 [==============================] - 0s 180us/step - loss: 0.5915 - acc: 0.6985\n",
      "Epoch 9/100\n",
      "2093/2093 [==============================] - 0s 178us/step - loss: 0.5866 - acc: 0.6976\n",
      "Epoch 10/100\n",
      "2093/2093 [==============================] - 0s 178us/step - loss: 0.5831 - acc: 0.6980\n",
      "Epoch 11/100\n",
      "2093/2093 [==============================] - 0s 211us/step - loss: 0.5804 - acc: 0.6985\n",
      "Epoch 12/100\n",
      "2093/2093 [==============================] - 0s 178us/step - loss: 0.5780 - acc: 0.6990\n",
      "Epoch 13/100\n",
      "2093/2093 [==============================] - 0s 207us/step - loss: 0.5759 - acc: 0.6995\n",
      "Epoch 14/100\n",
      "2093/2093 [==============================] - 0s 187us/step - loss: 0.5740 - acc: 0.6995\n",
      "Epoch 15/100\n",
      "2093/2093 [==============================] - 0s 198us/step - loss: 0.5723 - acc: 0.7004\n",
      "Epoch 16/100\n",
      "2093/2093 [==============================] - 0s 222us/step - loss: 0.5708 - acc: 0.7004\n",
      "Epoch 17/100\n",
      "2093/2093 [==============================] - 0s 207us/step - loss: 0.5695 - acc: 0.6985\n",
      "Epoch 18/100\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5684 - acc: 0.6947\n",
      "Epoch 19/100\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5674 - acc: 0.6961\n",
      "Epoch 20/100\n",
      "2093/2093 [==============================] - 0s 213us/step - loss: 0.5666 - acc: 0.6961\n",
      "Epoch 21/100\n",
      "2093/2093 [==============================] - 0s 191us/step - loss: 0.5660 - acc: 0.6961\n",
      "Epoch 22/100\n",
      "2093/2093 [==============================] - 0s 183us/step - loss: 0.5655 - acc: 0.6957\n",
      "Epoch 23/100\n",
      "2093/2093 [==============================] - 0s 186us/step - loss: 0.5651 - acc: 0.6966\n",
      "Epoch 24/100\n",
      "2093/2093 [==============================] - 0s 191us/step - loss: 0.5647 - acc: 0.6966\n",
      "Epoch 25/100\n",
      "2093/2093 [==============================] - 0s 184us/step - loss: 0.5644 - acc: 0.6966\n",
      "Epoch 26/100\n",
      "2093/2093 [==============================] - 0s 198us/step - loss: 0.5642 - acc: 0.6971\n",
      "Epoch 27/100\n",
      "2093/2093 [==============================] - 0s 189us/step - loss: 0.5640 - acc: 0.6971\n",
      "Epoch 28/100\n",
      "2093/2093 [==============================] - 0s 187us/step - loss: 0.5638 - acc: 0.6971\n",
      "Epoch 29/100\n",
      "2093/2093 [==============================] - 0s 200us/step - loss: 0.5637 - acc: 0.6971\n",
      "Epoch 30/100\n",
      "2093/2093 [==============================] - 0s 219us/step - loss: 0.5636 - acc: 0.6971\n",
      "Epoch 31/100\n",
      "2093/2093 [==============================] - 0s 217us/step - loss: 0.5635 - acc: 0.6971\n",
      "Epoch 32/100\n",
      "2093/2093 [==============================] - 0s 195us/step - loss: 0.5634 - acc: 0.6971\n",
      "Epoch 33/100\n",
      "2093/2093 [==============================] - 0s 228us/step - loss: 0.5633 - acc: 0.6971\n",
      "Epoch 34/100\n",
      "2093/2093 [==============================] - 0s 214us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 35/100\n",
      "2093/2093 [==============================] - 0s 192us/step - loss: 0.5632 - acc: 0.6971\n",
      "Epoch 36/100\n",
      "2093/2093 [==============================] - 0s 212us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 37/100\n",
      "2093/2093 [==============================] - 0s 204us/step - loss: 0.5631 - acc: 0.6971\n",
      "Epoch 38/100\n",
      "2093/2093 [==============================] - 0s 200us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 39/100\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 40/100\n",
      "2093/2093 [==============================] - 0s 193us/step - loss: 0.5630 - acc: 0.6971\n",
      "Epoch 41/100\n",
      "2093/2093 [==============================] - 0s 190us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 42/100\n",
      "2093/2093 [==============================] - 0s 195us/step - loss: 0.5629 - acc: 0.6971\n",
      "Epoch 43/100\n",
      "2093/2093 [==============================] - 0s 204us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 44/100\n",
      "2093/2093 [==============================] - 0s 201us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 45/100\n",
      "2093/2093 [==============================] - 0s 196us/step - loss: 0.5628 - acc: 0.6971\n",
      "Epoch 46/100\n",
      "2093/2093 [==============================] - 0s 201us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 47/100\n",
      "2093/2093 [==============================] - 0s 188us/step - loss: 0.5627 - acc: 0.6971\n",
      "Epoch 48/100\n",
      "2093/2093 [==============================] - 0s 208us/step - loss: 0.5627 - acc: 0.6957\n",
      "Epoch 49/100\n",
      "2093/2093 [==============================] - 0s 187us/step - loss: 0.5626 - acc: 0.6957\n",
      "Epoch 50/100\n",
      "2093/2093 [==============================] - 0s 196us/step - loss: 0.5626 - acc: 0.6957\n",
      "Epoch 51/100\n",
      "2093/2093 [==============================] - 0s 227us/step - loss: 0.5626 - acc: 0.6957\n",
      "Epoch 52/100\n",
      "2093/2093 [==============================] - 0s 191us/step - loss: 0.5625 - acc: 0.6957\n",
      "Epoch 53/100\n",
      "2093/2093 [==============================] - 0s 228us/step - loss: 0.5625 - acc: 0.6957\n",
      "Epoch 54/100\n",
      "2093/2093 [==============================] - 0s 213us/step - loss: 0.5625 - acc: 0.6957\n",
      "Epoch 55/100\n",
      "2093/2093 [==============================] - 0s 193us/step - loss: 0.5624 - acc: 0.6957\n",
      "Epoch 56/100\n",
      "2093/2093 [==============================] - 0s 198us/step - loss: 0.5624 - acc: 0.6957\n",
      "Epoch 57/100\n",
      "2093/2093 [==============================] - 0s 210us/step - loss: 0.5624 - acc: 0.6971\n",
      "Epoch 58/100\n",
      "2093/2093 [==============================] - 0s 198us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 59/100\n",
      "2093/2093 [==============================] - 0s 207us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 60/100\n",
      "2093/2093 [==============================] - 0s 192us/step - loss: 0.5623 - acc: 0.6971\n",
      "Epoch 61/100\n",
      "2093/2093 [==============================] - 0s 218us/step - loss: 0.5622 - acc: 0.6971\n",
      "Epoch 62/100\n",
      "1504/2093 [====================>.........] - ETA: 0s - loss: 0.5656 - acc: 0.6968"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b1f15f4b97f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         classifier.fit(X_train, y_train, batch_size=bs, epochs=ep,\n\u001b[0;32m----> 5\u001b[0;31m                        class_weight='balanced', shuffle=False)\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for bs in [20, 16, 12, 8, 6]:\n",
    "    for ep in [50, 100, 200]:\n",
    "        classifier = _classifier()\n",
    "        classifier.fit(X_train, y_train, batch_size=bs, epochs=ep,\n",
    "                       class_weight='balanced', shuffle=False)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        y_pred = (y_pred > 0.5)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        results[(f'bs:{bs}, ep:{ep} (cm)')] = cm\n",
    "        results[(f'bs:{bs}, ep:{ep} (accuracy)')\n",
    "                ] = f'{((cm[0][0]+cm[1][1])/cm.sum()*100).round(1)}%'\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the best & the worst combinations:\n",
    "\n",
    "### The best:\n",
    "1. batch Size = 12, Epochs = 50\n",
    "    - CM = [__11__ 2][4 __13__]\n",
    "    - Accuracy = 80%\n",
    "2. batch Size = 12, Epochs = 100\n",
    "    - CM = [__11__ 2][4 __13__]\n",
    "    - Accuracy = 80%\n",
    "3. batch Size = 20, Epochs = 100\n",
    "    - CM = [__11__ 2][4 __13__]\n",
    "    - Accuracy = 80%\n",
    "4. batch Size = 8, Epochs = 50\n",
    "    - CM = [10 3][4 __13__]\n",
    "    - Accuracy = 76.67%\n",
    "\n",
    "### The worst:\n",
    "1. batch Size = 8, Epochs = 100\n",
    "    - CM = [11 2] [__7__ 10]\n",
    "    - Accuracy = 70%\n",
    "2. batch Size = 16, Epochs = 50\n",
    "    - CM = [10 3] [__6__ 11]\n",
    "    - Accuracy = 70%\n",
    "\n",
    "#### The rest were at 73.33%\n",
    "\n",
    "NOTE:\n",
    "- All batches of 6 were at 73%\n",
    "- All Epochs of 200 were at 73%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T22:25:09.053277Z",
     "start_time": "2018-11-20T22:24:27.423457Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2067/2067 [==============================] - 1s 443us/step - loss: 0.5001 - acc: 0.4954\n",
      "Epoch 2/100\n",
      "2067/2067 [==============================] - 0s 165us/step - loss: 0.4967 - acc: 0.5317\n",
      "Epoch 3/100\n",
      "2067/2067 [==============================] - 0s 160us/step - loss: 0.4549 - acc: 0.6923\n",
      "Epoch 4/100\n",
      "2067/2067 [==============================] - 0s 181us/step - loss: 0.4208 - acc: 0.7025\n",
      "Epoch 5/100\n",
      "2067/2067 [==============================] - 0s 169us/step - loss: 0.3986 - acc: 0.7112\n",
      "Epoch 6/100\n",
      "2067/2067 [==============================] - 0s 193us/step - loss: 0.3822 - acc: 0.7121\n",
      "Epoch 7/100\n",
      "2067/2067 [==============================] - 0s 169us/step - loss: 0.3695 - acc: 0.7097\n",
      "Epoch 8/100\n",
      "2067/2067 [==============================] - 0s 183us/step - loss: 0.3589 - acc: 0.7112\n",
      "Epoch 9/100\n",
      "2067/2067 [==============================] - 0s 216us/step - loss: 0.3472 - acc: 0.7194\n",
      "Epoch 10/100\n",
      "2067/2067 [==============================] - 0s 170us/step - loss: 0.3398 - acc: 0.7218\n",
      "Epoch 11/100\n",
      "2067/2067 [==============================] - 0s 180us/step - loss: 0.3322 - acc: 0.7223\n",
      "Epoch 12/100\n",
      "2067/2067 [==============================] - 0s 165us/step - loss: 0.3285 - acc: 0.7165\n",
      "Epoch 13/100\n",
      "2067/2067 [==============================] - 0s 164us/step - loss: 0.3212 - acc: 0.7194\n",
      "Epoch 14/100\n",
      "2067/2067 [==============================] - 0s 173us/step - loss: 0.3143 - acc: 0.7238\n",
      "Epoch 15/100\n",
      "2067/2067 [==============================] - 0s 207us/step - loss: 0.3043 - acc: 0.7320\n",
      "Epoch 16/100\n",
      "2067/2067 [==============================] - 0s 184us/step - loss: 0.2969 - acc: 0.7373\n",
      "Epoch 17/100\n",
      "2067/2067 [==============================] - 0s 222us/step - loss: 0.2916 - acc: 0.7392\n",
      "Epoch 18/100\n",
      "2067/2067 [==============================] - 0s 185us/step - loss: 0.2853 - acc: 0.7446\n",
      "Epoch 19/100\n",
      "2067/2067 [==============================] - 0s 209us/step - loss: 0.2814 - acc: 0.7450\n",
      "Epoch 20/100\n",
      "2067/2067 [==============================] - 1s 252us/step - loss: 0.2834 - acc: 0.7402\n",
      "Epoch 21/100\n",
      "2067/2067 [==============================] - 0s 200us/step - loss: 0.2774 - acc: 0.7460\n",
      "Epoch 22/100\n",
      "2067/2067 [==============================] - 0s 211us/step - loss: 0.2747 - acc: 0.7455\n",
      "Epoch 23/100\n",
      "2067/2067 [==============================] - 0s 195us/step - loss: 0.2691 - acc: 0.7518\n",
      "Epoch 24/100\n",
      "2067/2067 [==============================] - 0s 214us/step - loss: 0.2669 - acc: 0.7523\n",
      "Epoch 25/100\n",
      "2067/2067 [==============================] - 0s 221us/step - loss: 0.2642 - acc: 0.7537\n",
      "Epoch 26/100\n",
      "2067/2067 [==============================] - 0s 219us/step - loss: 0.2599 - acc: 0.7562\n",
      "Epoch 27/100\n",
      "2067/2067 [==============================] - 0s 201us/step - loss: 0.2576 - acc: 0.7567\n",
      "Epoch 28/100\n",
      "2067/2067 [==============================] - 0s 214us/step - loss: 0.2571 - acc: 0.7571\n",
      "Epoch 29/100\n",
      "2067/2067 [==============================] - 0s 228us/step - loss: 0.2517 - acc: 0.7610\n",
      "Epoch 30/100\n",
      "2067/2067 [==============================] - 0s 214us/step - loss: 0.2527 - acc: 0.7596\n",
      "Epoch 31/100\n",
      "2067/2067 [==============================] - 0s 194us/step - loss: 0.2555 - acc: 0.7552\n",
      "Epoch 32/100\n",
      "2067/2067 [==============================] - 0s 213us/step - loss: 0.2528 - acc: 0.7576\n",
      "Epoch 33/100\n",
      "2067/2067 [==============================] - 0s 210us/step - loss: 0.2500 - acc: 0.7610\n",
      "Epoch 34/100\n",
      "2067/2067 [==============================] - 0s 210us/step - loss: 0.2504 - acc: 0.7591\n",
      "Epoch 35/100\n",
      "2067/2067 [==============================] - 0s 212us/step - loss: 0.2496 - acc: 0.7591\n",
      "Epoch 36/100\n",
      "2067/2067 [==============================] - 0s 210us/step - loss: 0.2438 - acc: 0.7658\n",
      "Epoch 37/100\n",
      "2067/2067 [==============================] - 0s 208us/step - loss: 0.2499 - acc: 0.7571\n",
      "Epoch 38/100\n",
      "2067/2067 [==============================] - 0s 201us/step - loss: 0.2414 - acc: 0.7663\n",
      "Epoch 39/100\n",
      "2067/2067 [==============================] - 0s 208us/step - loss: 0.2427 - acc: 0.7639\n",
      "Epoch 40/100\n",
      "2067/2067 [==============================] - 0s 211us/step - loss: 0.2372 - acc: 0.7692\n",
      "Epoch 41/100\n",
      "2067/2067 [==============================] - 0s 185us/step - loss: 0.2378 - acc: 0.7678\n",
      "Epoch 42/100\n",
      "2067/2067 [==============================] - 1s 252us/step - loss: 0.2367 - acc: 0.7683\n",
      "Epoch 43/100\n",
      "2067/2067 [==============================] - 0s 187us/step - loss: 0.2377 - acc: 0.7678\n",
      "Epoch 44/100\n",
      "2067/2067 [==============================] - 1s 268us/step - loss: 0.2361 - acc: 0.7702\n",
      "Epoch 45/100\n",
      "2067/2067 [==============================] - 0s 221us/step - loss: 0.2357 - acc: 0.7692\n",
      "Epoch 46/100\n",
      "2067/2067 [==============================] - 0s 190us/step - loss: 0.2366 - acc: 0.7673\n",
      "Epoch 47/100\n",
      "2067/2067 [==============================] - 0s 183us/step - loss: 0.2298 - acc: 0.7755\n",
      "Epoch 48/100\n",
      "2067/2067 [==============================] - 0s 182us/step - loss: 0.2306 - acc: 0.7736\n",
      "Epoch 49/100\n",
      "2067/2067 [==============================] - 0s 172us/step - loss: 0.2286 - acc: 0.7755\n",
      "Epoch 50/100\n",
      "2067/2067 [==============================] - 0s 186us/step - loss: 0.2297 - acc: 0.7755\n",
      "Epoch 51/100\n",
      "2067/2067 [==============================] - 0s 191us/step - loss: 0.2286 - acc: 0.7750\n",
      "Epoch 52/100\n",
      "2067/2067 [==============================] - 0s 193us/step - loss: 0.2294 - acc: 0.7760\n",
      "Epoch 53/100\n",
      "2067/2067 [==============================] - 0s 175us/step - loss: 0.2321 - acc: 0.7712\n",
      "Epoch 54/100\n",
      "2067/2067 [==============================] - 0s 218us/step - loss: 0.2290 - acc: 0.7736\n",
      "Epoch 55/100\n",
      "2067/2067 [==============================] - 0s 181us/step - loss: 0.2301 - acc: 0.7721\n",
      "Epoch 56/100\n",
      "2067/2067 [==============================] - 0s 183us/step - loss: 0.2300 - acc: 0.7731\n",
      "Epoch 57/100\n",
      "2067/2067 [==============================] - 0s 195us/step - loss: 0.2253 - acc: 0.7770\n",
      "Epoch 58/100\n",
      "2067/2067 [==============================] - 0s 174us/step - loss: 0.2277 - acc: 0.7746\n",
      "Epoch 59/100\n",
      "2067/2067 [==============================] - 0s 196us/step - loss: 0.2311 - acc: 0.7716\n",
      "Epoch 60/100\n",
      "2067/2067 [==============================] - 0s 176us/step - loss: 0.2262 - acc: 0.7770\n",
      "Epoch 61/100\n",
      "2067/2067 [==============================] - 0s 185us/step - loss: 0.2276 - acc: 0.7746\n",
      "Epoch 62/100\n",
      "2067/2067 [==============================] - 0s 223us/step - loss: 0.2249 - acc: 0.7789\n",
      "Epoch 63/100\n",
      "2067/2067 [==============================] - 0s 181us/step - loss: 0.2248 - acc: 0.7779\n",
      "Epoch 64/100\n",
      "2067/2067 [==============================] - 0s 185us/step - loss: 0.2278 - acc: 0.7731\n",
      "Epoch 65/100\n",
      "2067/2067 [==============================] - 0s 175us/step - loss: 0.2281 - acc: 0.7731\n",
      "Epoch 66/100\n",
      "2067/2067 [==============================] - 0s 196us/step - loss: 0.2264 - acc: 0.7765\n",
      "Epoch 67/100\n",
      "2067/2067 [==============================] - 0s 181us/step - loss: 0.2260 - acc: 0.7760\n",
      "Epoch 68/100\n",
      "2067/2067 [==============================] - 0s 199us/step - loss: 0.2257 - acc: 0.7765\n",
      "Epoch 69/100\n",
      "2067/2067 [==============================] - 0s 217us/step - loss: 0.2266 - acc: 0.7755\n",
      "Epoch 70/100\n",
      "2067/2067 [==============================] - 0s 178us/step - loss: 0.2289 - acc: 0.7731\n",
      "Epoch 71/100\n",
      "2067/2067 [==============================] - 0s 184us/step - loss: 0.2299 - acc: 0.7721\n",
      "Epoch 72/100\n",
      "2067/2067 [==============================] - 0s 176us/step - loss: 0.2277 - acc: 0.7731\n",
      "Epoch 73/100\n",
      "2067/2067 [==============================] - 0s 176us/step - loss: 0.2253 - acc: 0.7760\n",
      "Epoch 74/100\n",
      "2067/2067 [==============================] - 0s 182us/step - loss: 0.2251 - acc: 0.7765\n",
      "Epoch 75/100\n",
      "2067/2067 [==============================] - 0s 191us/step - loss: 0.2261 - acc: 0.7750\n",
      "Epoch 76/100\n",
      "2067/2067 [==============================] - 0s 196us/step - loss: 0.2303 - acc: 0.7707\n",
      "Epoch 77/100\n",
      "2067/2067 [==============================] - 0s 225us/step - loss: 0.2243 - acc: 0.7784\n",
      "Epoch 78/100\n",
      "2067/2067 [==============================] - 1s 251us/step - loss: 0.2229 - acc: 0.7779\n",
      "Epoch 79/100\n",
      "2067/2067 [==============================] - 1s 250us/step - loss: 0.2192 - acc: 0.7823\n",
      "Epoch 80/100\n",
      "2067/2067 [==============================] - 0s 182us/step - loss: 0.2213 - acc: 0.7799\n",
      "Epoch 81/100\n",
      "2067/2067 [==============================] - 0s 239us/step - loss: 0.2293 - acc: 0.7702\n",
      "Epoch 82/100\n",
      "2067/2067 [==============================] - 0s 211us/step - loss: 0.2257 - acc: 0.7750\n",
      "Epoch 83/100\n",
      "2067/2067 [==============================] - 0s 185us/step - loss: 0.2262 - acc: 0.7741\n",
      "Epoch 84/100\n",
      "2067/2067 [==============================] - 0s 199us/step - loss: 0.2277 - acc: 0.7731\n",
      "Epoch 85/100\n",
      "2067/2067 [==============================] - 0s 161us/step - loss: 0.2260 - acc: 0.7746\n",
      "Epoch 86/100\n",
      "2067/2067 [==============================] - 0s 198us/step - loss: 0.2224 - acc: 0.7789\n",
      "Epoch 87/100\n",
      "2067/2067 [==============================] - 0s 162us/step - loss: 0.2221 - acc: 0.7794\n",
      "Epoch 88/100\n",
      "2067/2067 [==============================] - 0s 193us/step - loss: 0.2213 - acc: 0.7794\n",
      "Epoch 89/100\n",
      "2067/2067 [==============================] - 0s 167us/step - loss: 0.2240 - acc: 0.7760\n",
      "Epoch 90/100\n",
      "2067/2067 [==============================] - 0s 173us/step - loss: 0.2241 - acc: 0.7770\n",
      "Epoch 91/100\n",
      "2067/2067 [==============================] - 0s 194us/step - loss: 0.2261 - acc: 0.7750\n",
      "Epoch 92/100\n",
      "2067/2067 [==============================] - 0s 200us/step - loss: 0.2244 - acc: 0.7770\n",
      "Epoch 93/100\n",
      "2067/2067 [==============================] - 0s 184us/step - loss: 0.2287 - acc: 0.7716\n",
      "Epoch 94/100\n",
      "2067/2067 [==============================] - 0s 186us/step - loss: 0.2301 - acc: 0.7702\n",
      "Epoch 95/100\n",
      "2067/2067 [==============================] - 0s 195us/step - loss: 0.2251 - acc: 0.7755\n",
      "Epoch 96/100\n",
      "2067/2067 [==============================] - 0s 202us/step - loss: 0.2258 - acc: 0.7750\n",
      "Epoch 97/100\n",
      "2067/2067 [==============================] - 0s 195us/step - loss: 0.2241 - acc: 0.7765\n",
      "Epoch 98/100\n",
      "2067/2067 [==============================] - 0s 192us/step - loss: 0.2205 - acc: 0.7794\n",
      "Epoch 99/100\n",
      "2067/2067 [==============================] - 0s 186us/step - loss: 0.2206 - acc: 0.7794\n",
      "Epoch 100/100\n",
      "2067/2067 [==============================] - 0s 190us/step - loss: 0.2185 - acc: 0.7823\n"
     ]
    }
   ],
   "source": [
    "classifier = _classifier()\n",
    "classifier.fit(X_train, y_train, batch_size=20, epochs=100,\n",
    "               class_weight='balanced', shuffle=False)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T22:25:09.076648Z",
     "start_time": "2018-11-20T22:25:09.055441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  6]\n",
      " [ 6 15]]\n",
      "accuracy: 67.6%\n"
     ]
    }
   ],
   "source": [
    "print(cm)\n",
    "print(f'accuracy: {((cm[0][0]+cm[1][1])/cm.sum()*100).round(1)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
