{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:35:53.555304Z",
     "start_time": "2018-11-18T23:35:53.530651Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import *\n",
    "from numpy import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.129413Z",
     "start_time": "2018-11-06T01:18:12.107495Z"
    }
   },
   "source": [
    "## Convert series to supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:35:54.743574Z",
     "start_time": "2018-11-18T23:35:54.716127Z"
    }
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:35:55.178131Z",
     "start_time": "2018-11-18T23:35:55.150741Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/use_for_analysis.csv')\n",
    "# df['start_time'] = df['start_time']//1000\n",
    "# df = df[df['result'] != 0.5]\n",
    "# numeric_predictors = ['color', 'diff', 'game_time', 'start_time', 'weekday']\n",
    "# X = df[numeric_predictors].values\n",
    "# y = np.array(df['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:37:18.556382Z",
     "start_time": "2018-11-18T23:37:18.519434Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['date', 'color', 'day', 'weekday', 'game_time', 'elo', 'diff', 'result']]\n",
    "df = df[df['result'] != 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:37:19.251851Z",
     "start_time": "2018-11-18T23:37:19.216566Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:6].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:39:53.288509Z",
     "start_time": "2018-11-18T23:39:53.265970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2098, 6)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:40:24.138467Z",
     "start_time": "2018-11-18T23:40:24.110932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1993.1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]*.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:40:44.929166Z",
     "start_time": "2018-11-18T23:40:44.907642Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X = X[:1993]\n",
    "test_X = X[1993:]\n",
    "\n",
    "train_Y = y[0:1993]\n",
    "test_Y = y[1993:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T23:44:07.325106Z",
     "start_time": "2018-11-18T23:44:07.287173Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_4: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-17cb6ff7722b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_4: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(18, return_sequences=True), input_shape=train_X.shape[1:]))\n",
    "model.add(Bidirectional(LSTM(12, return_sequences=True)))\n",
    "model.add(Dense(6))\n",
    "model.compile(loss='mae', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.226648Z",
     "start_time": "2018-11-06T01:18:12.191982Z"
    }
   },
   "outputs": [],
   "source": [
    "# # integer encode direction\n",
    "# encoder = LabelEncoder()\n",
    "# X = encoder.fit_transform(X)\n",
    "# # y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.271315Z",
     "start_time": "2018-11-06T01:18:12.252484Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.292315Z",
     "start_time": "2018-11-06T01:18:12.273196Z"
    }
   },
   "outputs": [],
   "source": [
    "# # specify the number of lag hours\n",
    "# n_hours = 3\n",
    "# n_features = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.316358Z",
     "start_time": "2018-11-06T01:18:12.294390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2075, 10)\n"
     ]
    }
   ],
   "source": [
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(X, 1, 1)\n",
    "print(reframed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.339518Z",
     "start_time": "2018-11-06T01:18:12.318482Z"
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.360426Z",
     "start_time": "2018-11-06T01:18:12.341383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1660, 5) 1660 (1660,)\n"
     ]
    }
   ],
   "source": [
    "# split into input and outputs\n",
    "# n_obs = n_hours * n_features\n",
    "# train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "# test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "print(X_train.shape, len(X_train), y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.383822Z",
     "start_time": "2018-11-06T01:18:12.362231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1660, 1, 5) (1660,) (416, 1, 5) (416,)\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:18:12.631634Z",
     "start_time": "2018-11-06T01:18:12.385707Z"
    }
   },
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:20:05.326693Z",
     "start_time": "2018-11-06T01:18:12.633307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1660 samples, validate on 416 samples\n",
      "Epoch 1/205\n",
      "1660/1660 [==============================] - 2s 978us/step - loss: 0.7959 - val_loss: 0.7008\n",
      "Epoch 2/205\n",
      "1660/1660 [==============================] - 1s 305us/step - loss: 0.6834 - val_loss: 0.6985\n",
      "Epoch 3/205\n",
      "1660/1660 [==============================] - 1s 310us/step - loss: 0.6804 - val_loss: 0.6956\n",
      "Epoch 4/205\n",
      "1660/1660 [==============================] - 1s 317us/step - loss: 0.6775 - val_loss: 0.6929\n",
      "Epoch 5/205\n",
      "1660/1660 [==============================] - 0s 297us/step - loss: 0.6749 - val_loss: 0.6903\n",
      "Epoch 6/205\n",
      "1660/1660 [==============================] - 1s 314us/step - loss: 0.6723 - val_loss: 0.6876\n",
      "Epoch 7/205\n",
      "1660/1660 [==============================] - 1s 335us/step - loss: 0.6697 - val_loss: 0.6849\n",
      "Epoch 8/205\n",
      "1660/1660 [==============================] - 1s 310us/step - loss: 0.6671 - val_loss: 0.6819\n",
      "Epoch 9/205\n",
      "1660/1660 [==============================] - 1s 332us/step - loss: 0.6643 - val_loss: 0.6787\n",
      "Epoch 10/205\n",
      "1660/1660 [==============================] - 1s 318us/step - loss: 0.6614 - val_loss: 0.6755\n",
      "Epoch 11/205\n",
      "1660/1660 [==============================] - 1s 314us/step - loss: 0.6585 - val_loss: 0.6724\n",
      "Epoch 12/205\n",
      "1660/1660 [==============================] - 1s 313us/step - loss: 0.6555 - val_loss: 0.6694\n",
      "Epoch 13/205\n",
      "1660/1660 [==============================] - 1s 316us/step - loss: 0.6526 - val_loss: 0.6666\n",
      "Epoch 14/205\n",
      "1660/1660 [==============================] - 0s 285us/step - loss: 0.6498 - val_loss: 0.6640\n",
      "Epoch 15/205\n",
      "1660/1660 [==============================] - 1s 309us/step - loss: 0.6472 - val_loss: 0.6615\n",
      "Epoch 16/205\n",
      "1660/1660 [==============================] - 1s 307us/step - loss: 0.6447 - val_loss: 0.6592\n",
      "Epoch 17/205\n",
      "1660/1660 [==============================] - 1s 303us/step - loss: 0.6422 - val_loss: 0.6569\n",
      "Epoch 18/205\n",
      "1660/1660 [==============================] - 1s 306us/step - loss: 0.6399 - val_loss: 0.6546\n",
      "Epoch 19/205\n",
      "1660/1660 [==============================] - 1s 320us/step - loss: 0.6380 - val_loss: 0.6523\n",
      "Epoch 20/205\n",
      "1660/1660 [==============================] - 1s 355us/step - loss: 0.6362 - val_loss: 0.6499\n",
      "Epoch 21/205\n",
      "1660/1660 [==============================] - 1s 303us/step - loss: 0.6347 - val_loss: 0.6474\n",
      "Epoch 22/205\n",
      "1660/1660 [==============================] - 1s 316us/step - loss: 0.6337 - val_loss: 0.6455\n",
      "Epoch 23/205\n",
      "1660/1660 [==============================] - 1s 307us/step - loss: 0.6329 - val_loss: 0.6443\n",
      "Epoch 24/205\n",
      "1660/1660 [==============================] - 1s 313us/step - loss: 0.6319 - val_loss: 0.6433\n",
      "Epoch 25/205\n",
      "1660/1660 [==============================] - 1s 303us/step - loss: 0.6312 - val_loss: 0.6428\n",
      "Epoch 26/205\n",
      "1660/1660 [==============================] - 0s 298us/step - loss: 0.6303 - val_loss: 0.6423\n",
      "Epoch 27/205\n",
      "1660/1660 [==============================] - 0s 298us/step - loss: 0.6299 - val_loss: 0.6420\n",
      "Epoch 28/205\n",
      "1660/1660 [==============================] - 1s 305us/step - loss: 0.6289 - val_loss: 0.6420\n",
      "Epoch 29/205\n",
      "1660/1660 [==============================] - 1s 320us/step - loss: 0.6291 - val_loss: 0.6416\n",
      "Epoch 30/205\n",
      "1660/1660 [==============================] - 1s 311us/step - loss: 0.6276 - val_loss: 0.6427\n",
      "Epoch 31/205\n",
      "1660/1660 [==============================] - 1s 304us/step - loss: 0.6295 - val_loss: 0.6413\n",
      "Epoch 32/205\n",
      "1660/1660 [==============================] - 1s 352us/step - loss: 0.6260 - val_loss: 0.6434\n",
      "Epoch 33/205\n",
      "1660/1660 [==============================] - 1s 314us/step - loss: 0.6292 - val_loss: 0.6410\n",
      "Epoch 34/205\n",
      "1660/1660 [==============================] - 1s 322us/step - loss: 0.6249 - val_loss: 0.6424\n",
      "Epoch 35/205\n",
      "1660/1660 [==============================] - 0s 299us/step - loss: 0.6267 - val_loss: 0.6413\n",
      "Epoch 36/205\n",
      "1660/1660 [==============================] - 1s 326us/step - loss: 0.6247 - val_loss: 0.6432\n",
      "Epoch 37/205\n",
      "1660/1660 [==============================] - 1s 303us/step - loss: 0.6270 - val_loss: 0.6411\n",
      "Epoch 38/205\n",
      "1660/1660 [==============================] - 1s 303us/step - loss: 0.6238 - val_loss: 0.6428\n",
      "Epoch 39/205\n",
      "1660/1660 [==============================] - 1s 308us/step - loss: 0.6256 - val_loss: 0.6414\n",
      "Epoch 40/205\n",
      "1660/1660 [==============================] - 1s 350us/step - loss: 0.6235 - val_loss: 0.6430\n",
      "Epoch 41/205\n",
      "1660/1660 [==============================] - 1s 354us/step - loss: 0.6252 - val_loss: 0.6414\n",
      "Epoch 42/205\n",
      "1660/1660 [==============================] - 1s 335us/step - loss: 0.6230 - val_loss: 0.6429\n",
      "Epoch 43/205\n",
      "1660/1660 [==============================] - 1s 325us/step - loss: 0.6244 - val_loss: 0.6417\n",
      "Epoch 44/205\n",
      "1660/1660 [==============================] - 1s 330us/step - loss: 0.6226 - val_loss: 0.6428\n",
      "Epoch 45/205\n",
      "1660/1660 [==============================] - 1s 305us/step - loss: 0.6237 - val_loss: 0.6421\n",
      "Epoch 46/205\n",
      "1660/1660 [==============================] - 1s 314us/step - loss: 0.6222 - val_loss: 0.6428\n",
      "Epoch 47/205\n",
      "1660/1660 [==============================] - 1s 360us/step - loss: 0.6230 - val_loss: 0.6427\n",
      "Epoch 48/205\n",
      "1660/1660 [==============================] - 1s 304us/step - loss: 0.6218 - val_loss: 0.6428\n",
      "Epoch 49/205\n",
      "1660/1660 [==============================] - 1s 337us/step - loss: 0.6224 - val_loss: 0.6437\n",
      "Epoch 50/205\n",
      "1660/1660 [==============================] - 1s 327us/step - loss: 0.6214 - val_loss: 0.6430\n",
      "Epoch 51/205\n",
      "1660/1660 [==============================] - 1s 319us/step - loss: 0.6218 - val_loss: 0.6469\n",
      "Epoch 52/205\n",
      "1660/1660 [==============================] - 1s 345us/step - loss: 0.6211 - val_loss: 0.6435\n",
      "Epoch 53/205\n",
      "1660/1660 [==============================] - 1s 344us/step - loss: 0.6210 - val_loss: 0.6683\n",
      "Epoch 54/205\n",
      "1660/1660 [==============================] - 1s 362us/step - loss: 0.6208 - val_loss: 0.6450\n",
      "Epoch 55/205\n",
      "1660/1660 [==============================] - 1s 321us/step - loss: 0.6204 - val_loss: 0.6680\n",
      "Epoch 56/205\n",
      "1660/1660 [==============================] - 1s 359us/step - loss: 0.6205 - val_loss: 0.6687\n",
      "Epoch 57/205\n",
      "1660/1660 [==============================] - 1s 339us/step - loss: 0.6199 - val_loss: 0.6677\n",
      "Epoch 58/205\n",
      "1660/1660 [==============================] - 1s 333us/step - loss: 0.6201 - val_loss: 0.6681\n",
      "Epoch 59/205\n",
      "1660/1660 [==============================] - 1s 310us/step - loss: 0.6193 - val_loss: 0.6673\n",
      "Epoch 60/205\n",
      "1660/1660 [==============================] - 1s 351us/step - loss: 0.6197 - val_loss: 0.6675\n",
      "Epoch 61/205\n",
      "1660/1660 [==============================] - 1s 341us/step - loss: 0.6188 - val_loss: 0.6669\n",
      "Epoch 62/205\n",
      "1660/1660 [==============================] - 1s 352us/step - loss: 0.6193 - val_loss: 0.6669\n",
      "Epoch 63/205\n",
      "1660/1660 [==============================] - 1s 354us/step - loss: 0.6184 - val_loss: 0.6666\n",
      "Epoch 64/205\n",
      "1660/1660 [==============================] - 1s 351us/step - loss: 0.6189 - val_loss: 0.6662\n",
      "Epoch 65/205\n",
      "1660/1660 [==============================] - 1s 367us/step - loss: 0.6180 - val_loss: 0.6663\n",
      "Epoch 66/205\n",
      "1660/1660 [==============================] - 1s 360us/step - loss: 0.6185 - val_loss: 0.6655\n",
      "Epoch 67/205\n",
      "1660/1660 [==============================] - 1s 383us/step - loss: 0.6177 - val_loss: 0.6662\n",
      "Epoch 68/205\n",
      "1660/1660 [==============================] - 1s 349us/step - loss: 0.6179 - val_loss: 0.6648\n",
      "Epoch 69/205\n",
      "1660/1660 [==============================] - 1s 430us/step - loss: 0.6178 - val_loss: 0.6657\n",
      "Epoch 70/205\n",
      "1660/1660 [==============================] - 1s 363us/step - loss: 0.6169 - val_loss: 0.6645\n",
      "Epoch 71/205\n",
      "1660/1660 [==============================] - 1s 321us/step - loss: 0.6178 - val_loss: 0.6651\n",
      "Epoch 72/205\n",
      "1660/1660 [==============================] - 1s 320us/step - loss: 0.6162 - val_loss: 0.6642\n",
      "Epoch 73/205\n",
      "1660/1660 [==============================] - 1s 338us/step - loss: 0.6175 - val_loss: 0.6644\n",
      "Epoch 74/205\n",
      "1660/1660 [==============================] - 1s 343us/step - loss: 0.6157 - val_loss: 0.6639\n",
      "Epoch 75/205\n",
      "1660/1660 [==============================] - 1s 327us/step - loss: 0.6170 - val_loss: 0.6637\n",
      "Epoch 76/205\n",
      "1660/1660 [==============================] - 1s 338us/step - loss: 0.6152 - val_loss: 0.6636\n",
      "Epoch 77/205\n",
      "1660/1660 [==============================] - 1s 331us/step - loss: 0.6166 - val_loss: 0.6631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/205\n",
      "1660/1660 [==============================] - 0s 292us/step - loss: 0.6148 - val_loss: 0.6631\n",
      "Epoch 79/205\n",
      "1660/1660 [==============================] - 1s 338us/step - loss: 0.6160 - val_loss: 0.6626\n",
      "Epoch 80/205\n",
      "1660/1660 [==============================] - 1s 318us/step - loss: 0.6145 - val_loss: 0.6626\n",
      "Epoch 81/205\n",
      "1660/1660 [==============================] - 1s 331us/step - loss: 0.6154 - val_loss: 0.6620\n",
      "Epoch 82/205\n",
      "1660/1660 [==============================] - 1s 323us/step - loss: 0.6142 - val_loss: 0.6621\n",
      "Epoch 83/205\n",
      "1660/1660 [==============================] - 1s 324us/step - loss: 0.6148 - val_loss: 0.6615\n",
      "Epoch 84/205\n",
      "1660/1660 [==============================] - 1s 335us/step - loss: 0.6140 - val_loss: 0.6616\n",
      "Epoch 85/205\n",
      "1660/1660 [==============================] - 1s 335us/step - loss: 0.6141 - val_loss: 0.6609\n",
      "Epoch 86/205\n",
      "1660/1660 [==============================] - 1s 317us/step - loss: 0.6139 - val_loss: 0.6611\n",
      "Epoch 87/205\n",
      "1660/1660 [==============================] - 1s 362us/step - loss: 0.6134 - val_loss: 0.6603\n",
      "Epoch 88/205\n",
      "1660/1660 [==============================] - 1s 306us/step - loss: 0.6141 - val_loss: 0.6608\n",
      "Epoch 89/205\n",
      "1660/1660 [==============================] - 1s 322us/step - loss: 0.6125 - val_loss: 0.6598\n",
      "Epoch 90/205\n",
      "1660/1660 [==============================] - 1s 328us/step - loss: 0.6143 - val_loss: 0.6606\n",
      "Epoch 91/205\n",
      "1660/1660 [==============================] - 1s 332us/step - loss: 0.6117 - val_loss: 0.6593\n",
      "Epoch 92/205\n",
      "1660/1660 [==============================] - 1s 323us/step - loss: 0.6138 - val_loss: 0.6599\n",
      "Epoch 93/205\n",
      "1660/1660 [==============================] - 1s 322us/step - loss: 0.6112 - val_loss: 0.6588\n",
      "Epoch 94/205\n",
      "1660/1660 [==============================] - 1s 325us/step - loss: 0.6134 - val_loss: 0.6594\n",
      "Epoch 95/205\n",
      "1660/1660 [==============================] - 1s 313us/step - loss: 0.6108 - val_loss: 0.6583\n",
      "Epoch 96/205\n",
      "1660/1660 [==============================] - 1s 347us/step - loss: 0.6129 - val_loss: 0.6589\n",
      "Epoch 97/205\n",
      "1660/1660 [==============================] - 1s 325us/step - loss: 0.6104 - val_loss: 0.6577\n",
      "Epoch 98/205\n",
      "1660/1660 [==============================] - 1s 342us/step - loss: 0.6125 - val_loss: 0.6584\n",
      "Epoch 99/205\n",
      "1660/1660 [==============================] - 1s 320us/step - loss: 0.6099 - val_loss: 0.6571\n",
      "Epoch 100/205\n",
      "1660/1660 [==============================] - 1s 329us/step - loss: 0.6121 - val_loss: 0.6579\n",
      "Epoch 101/205\n",
      "1660/1660 [==============================] - 1s 334us/step - loss: 0.6095 - val_loss: 0.6566\n",
      "Epoch 102/205\n",
      "1660/1660 [==============================] - 1s 330us/step - loss: 0.6116 - val_loss: 0.6574\n",
      "Epoch 103/205\n",
      "1660/1660 [==============================] - 1s 337us/step - loss: 0.6091 - val_loss: 0.6560\n",
      "Epoch 104/205\n",
      "1660/1660 [==============================] - 1s 342us/step - loss: 0.6112 - val_loss: 0.6569\n",
      "Epoch 105/205\n",
      "1660/1660 [==============================] - 1s 308us/step - loss: 0.6086 - val_loss: 0.6555\n",
      "Epoch 106/205\n",
      "1660/1660 [==============================] - 1s 326us/step - loss: 0.6107 - val_loss: 0.6562\n",
      "Epoch 107/205\n",
      "1660/1660 [==============================] - 1s 337us/step - loss: 0.6082 - val_loss: 0.6549\n",
      "Epoch 108/205\n",
      "1660/1660 [==============================] - 1s 327us/step - loss: 0.6103 - val_loss: 0.6558\n",
      "Epoch 109/205\n",
      "1660/1660 [==============================] - 1s 345us/step - loss: 0.6078 - val_loss: 0.6544\n",
      "Epoch 110/205\n",
      "1660/1660 [==============================] - 1s 340us/step - loss: 0.6097 - val_loss: 0.6553\n",
      "Epoch 111/205\n",
      "1660/1660 [==============================] - 1s 352us/step - loss: 0.6074 - val_loss: 0.6539\n",
      "Epoch 112/205\n",
      "1660/1660 [==============================] - 1s 307us/step - loss: 0.6093 - val_loss: 0.6548\n",
      "Epoch 113/205\n",
      "1660/1660 [==============================] - 1s 352us/step - loss: 0.6069 - val_loss: 0.6533\n",
      "Epoch 114/205\n",
      "1660/1660 [==============================] - 1s 327us/step - loss: 0.6088 - val_loss: 0.6543\n",
      "Epoch 115/205\n",
      "1660/1660 [==============================] - 1s 323us/step - loss: 0.6065 - val_loss: 0.6528\n",
      "Epoch 116/205\n",
      "1660/1660 [==============================] - 1s 361us/step - loss: 0.6084 - val_loss: 0.6538\n",
      "Epoch 117/205\n",
      "1660/1660 [==============================] - 1s 327us/step - loss: 0.6061 - val_loss: 0.6523\n",
      "Epoch 118/205\n",
      "1660/1660 [==============================] - 1s 330us/step - loss: 0.6079 - val_loss: 0.6532\n",
      "Epoch 119/205\n",
      "1660/1660 [==============================] - 1s 318us/step - loss: 0.6057 - val_loss: 0.6517\n",
      "Epoch 120/205\n",
      "1660/1660 [==============================] - 1s 342us/step - loss: 0.6075 - val_loss: 0.6527\n",
      "Epoch 121/205\n",
      "1660/1660 [==============================] - 1s 330us/step - loss: 0.6053 - val_loss: 0.6512\n",
      "Epoch 122/205\n",
      "1660/1660 [==============================] - 1s 339us/step - loss: 0.6071 - val_loss: 0.6521\n",
      "Epoch 123/205\n",
      "1660/1660 [==============================] - 1s 312us/step - loss: 0.6048 - val_loss: 0.6506\n",
      "Epoch 124/205\n",
      "1660/1660 [==============================] - 1s 334us/step - loss: 0.6066 - val_loss: 0.6516\n",
      "Epoch 125/205\n",
      "1660/1660 [==============================] - 1s 316us/step - loss: 0.6044 - val_loss: 0.6501\n",
      "Epoch 126/205\n",
      "1660/1660 [==============================] - 1s 321us/step - loss: 0.6061 - val_loss: 0.6509\n",
      "Epoch 127/205\n",
      "1660/1660 [==============================] - 1s 318us/step - loss: 0.6040 - val_loss: 0.6495\n",
      "Epoch 128/205\n",
      "1660/1660 [==============================] - 1s 345us/step - loss: 0.6057 - val_loss: 0.6505\n",
      "Epoch 129/205\n",
      "1660/1660 [==============================] - 1s 324us/step - loss: 0.6036 - val_loss: 0.6490\n",
      "Epoch 130/205\n",
      "1660/1660 [==============================] - 1s 329us/step - loss: 0.6052 - val_loss: 0.6498\n",
      "Epoch 131/205\n",
      "1660/1660 [==============================] - 1s 335us/step - loss: 0.6031 - val_loss: 0.6484\n",
      "Epoch 132/205\n",
      "1660/1660 [==============================] - 1s 314us/step - loss: 0.6047 - val_loss: 0.6492\n",
      "Epoch 133/205\n",
      "1660/1660 [==============================] - 1s 339us/step - loss: 0.6027 - val_loss: 0.6479\n",
      "Epoch 134/205\n",
      "1660/1660 [==============================] - 1s 315us/step - loss: 0.6042 - val_loss: 0.6487\n",
      "Epoch 135/205\n",
      "1660/1660 [==============================] - 1s 328us/step - loss: 0.6023 - val_loss: 0.6473\n",
      "Epoch 136/205\n",
      "1660/1660 [==============================] - 1s 355us/step - loss: 0.6037 - val_loss: 0.6481\n",
      "Epoch 137/205\n",
      "1660/1660 [==============================] - 1s 324us/step - loss: 0.6018 - val_loss: 0.6468\n",
      "Epoch 138/205\n",
      "1660/1660 [==============================] - 1s 352us/step - loss: 0.6032 - val_loss: 0.6476\n",
      "Epoch 139/205\n",
      "1660/1660 [==============================] - 1s 323us/step - loss: 0.6013 - val_loss: 0.6463\n",
      "Epoch 140/205\n",
      "1660/1660 [==============================] - 1s 322us/step - loss: 0.6027 - val_loss: 0.6470\n",
      "Epoch 141/205\n",
      "1660/1660 [==============================] - 1s 341us/step - loss: 0.6008 - val_loss: 0.6458\n",
      "Epoch 142/205\n",
      "1660/1660 [==============================] - 1s 318us/step - loss: 0.6022 - val_loss: 0.6465\n",
      "Epoch 143/205\n",
      "1660/1660 [==============================] - 1s 350us/step - loss: 0.6003 - val_loss: 0.6454\n",
      "Epoch 144/205\n",
      "1660/1660 [==============================] - 1s 310us/step - loss: 0.6015 - val_loss: 0.6459\n",
      "Epoch 145/205\n",
      "1660/1660 [==============================] - 1s 329us/step - loss: 0.6000 - val_loss: 0.6449\n",
      "Epoch 146/205\n",
      "1660/1660 [==============================] - 1s 318us/step - loss: 0.6011 - val_loss: 0.6455\n",
      "Epoch 147/205\n",
      "1660/1660 [==============================] - 1s 332us/step - loss: 0.5995 - val_loss: 0.6445\n",
      "Epoch 148/205\n",
      "1660/1660 [==============================] - 1s 321us/step - loss: 0.6006 - val_loss: 0.6451\n",
      "Epoch 149/205\n",
      "1660/1660 [==============================] - 1s 353us/step - loss: 0.5989 - val_loss: 0.6440\n",
      "Epoch 150/205\n",
      "1660/1660 [==============================] - 1s 317us/step - loss: 0.6002 - val_loss: 0.6447\n",
      "Epoch 151/205\n",
      "1660/1660 [==============================] - 1s 338us/step - loss: 0.5985 - val_loss: 0.6436\n",
      "Epoch 152/205\n",
      "1660/1660 [==============================] - 1s 328us/step - loss: 0.5996 - val_loss: 0.6441\n",
      "Epoch 153/205\n",
      "1660/1660 [==============================] - 1s 338us/step - loss: 0.5981 - val_loss: 0.6432\n",
      "Epoch 154/205\n",
      "1660/1660 [==============================] - 1s 335us/step - loss: 0.5990 - val_loss: 0.6436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/205\n",
      "1660/1660 [==============================] - 1s 303us/step - loss: 0.5976 - val_loss: 0.6429\n",
      "Epoch 156/205\n",
      "1660/1660 [==============================] - 1s 305us/step - loss: 0.5986 - val_loss: 0.6433\n",
      "Epoch 157/205\n",
      "1660/1660 [==============================] - 1s 302us/step - loss: 0.5971 - val_loss: 0.6425\n",
      "Epoch 158/205\n",
      "1660/1660 [==============================] - 1s 311us/step - loss: 0.5981 - val_loss: 0.6429\n",
      "Epoch 159/205\n",
      "1660/1660 [==============================] - 1s 318us/step - loss: 0.5966 - val_loss: 0.6422\n",
      "Epoch 160/205\n",
      "1660/1660 [==============================] - 1s 313us/step - loss: 0.5976 - val_loss: 0.6426\n",
      "Epoch 161/205\n",
      "1660/1660 [==============================] - 1s 319us/step - loss: 0.5962 - val_loss: 0.6419\n",
      "Epoch 162/205\n",
      "1660/1660 [==============================] - 1s 307us/step - loss: 0.5971 - val_loss: 0.6423\n",
      "Epoch 163/205\n",
      "1660/1660 [==============================] - 1s 306us/step - loss: 0.5958 - val_loss: 0.6417\n",
      "Epoch 164/205\n",
      "1660/1660 [==============================] - 1s 314us/step - loss: 0.5965 - val_loss: 0.6419\n",
      "Epoch 165/205\n",
      "1660/1660 [==============================] - 1s 334us/step - loss: 0.5955 - val_loss: 0.6415\n",
      "Epoch 166/205\n",
      "1660/1660 [==============================] - 1s 331us/step - loss: 0.5958 - val_loss: 0.6417\n",
      "Epoch 167/205\n",
      "1660/1660 [==============================] - 1s 302us/step - loss: 0.5952 - val_loss: 0.6414\n",
      "Epoch 168/205\n",
      "1660/1660 [==============================] - 1s 313us/step - loss: 0.5952 - val_loss: 0.6414\n",
      "Epoch 169/205\n",
      "1660/1660 [==============================] - 1s 322us/step - loss: 0.5949 - val_loss: 0.6412\n",
      "Epoch 170/205\n",
      "1660/1660 [==============================] - 1s 309us/step - loss: 0.5947 - val_loss: 0.6413\n",
      "Epoch 171/205\n",
      "1660/1660 [==============================] - 1s 313us/step - loss: 0.5944 - val_loss: 0.6411\n",
      "Epoch 172/205\n",
      "1660/1660 [==============================] - 1s 309us/step - loss: 0.5943 - val_loss: 0.6412\n",
      "Epoch 173/205\n",
      "1660/1660 [==============================] - 1s 310us/step - loss: 0.5939 - val_loss: 0.6411\n",
      "Epoch 174/205\n",
      "1660/1660 [==============================] - 1s 309us/step - loss: 0.5938 - val_loss: 0.6411\n",
      "Epoch 175/205\n",
      "1660/1660 [==============================] - 1s 313us/step - loss: 0.5936 - val_loss: 0.6410\n",
      "Epoch 176/205\n",
      "1660/1660 [==============================] - 1s 323us/step - loss: 0.5933 - val_loss: 0.6411\n",
      "Epoch 177/205\n",
      "1660/1660 [==============================] - 1s 304us/step - loss: 0.5932 - val_loss: 0.6409\n",
      "Epoch 178/205\n",
      "1660/1660 [==============================] - 1s 303us/step - loss: 0.5928 - val_loss: 0.6410\n",
      "Epoch 179/205\n",
      "1660/1660 [==============================] - 1s 307us/step - loss: 0.5928 - val_loss: 0.6407\n",
      "Epoch 180/205\n",
      "1660/1660 [==============================] - 1s 317us/step - loss: 0.5923 - val_loss: 0.6410\n",
      "Epoch 181/205\n",
      "1660/1660 [==============================] - 1s 317us/step - loss: 0.5924 - val_loss: 0.6404\n",
      "Epoch 182/205\n",
      "1660/1660 [==============================] - 1s 323us/step - loss: 0.5919 - val_loss: 0.6410\n",
      "Epoch 183/205\n",
      "1660/1660 [==============================] - 1s 322us/step - loss: 0.5919 - val_loss: 0.6404\n",
      "Epoch 184/205\n",
      "1660/1660 [==============================] - 1s 322us/step - loss: 0.5916 - val_loss: 0.6409\n",
      "Epoch 185/205\n",
      "1660/1660 [==============================] - 1s 320us/step - loss: 0.5914 - val_loss: 0.6405\n",
      "Epoch 186/205\n",
      "1660/1660 [==============================] - 1s 328us/step - loss: 0.5913 - val_loss: 0.6407\n",
      "Epoch 187/205\n",
      "1660/1660 [==============================] - 1s 361us/step - loss: 0.5910 - val_loss: 0.6407\n",
      "Epoch 188/205\n",
      "1660/1660 [==============================] - 1s 332us/step - loss: 0.5909 - val_loss: 0.6406\n",
      "Epoch 189/205\n",
      "1660/1660 [==============================] - 1s 307us/step - loss: 0.5906 - val_loss: 0.6410\n",
      "Epoch 190/205\n",
      "1660/1660 [==============================] - 1s 314us/step - loss: 0.5906 - val_loss: 0.6402\n",
      "Epoch 191/205\n",
      "1660/1660 [==============================] - 1s 304us/step - loss: 0.5901 - val_loss: 0.6415\n",
      "Epoch 192/205\n",
      "1660/1660 [==============================] - 0s 290us/step - loss: 0.5903 - val_loss: 0.6398\n",
      "Epoch 193/205\n",
      "1660/1660 [==============================] - 1s 310us/step - loss: 0.5897 - val_loss: 0.6424\n",
      "Epoch 194/205\n",
      "1660/1660 [==============================] - 0s 298us/step - loss: 0.5900 - val_loss: 0.6388\n",
      "Epoch 195/205\n",
      "1660/1660 [==============================] - 1s 303us/step - loss: 0.5893 - val_loss: 0.6439\n",
      "Epoch 196/205\n",
      "1660/1660 [==============================] - 1s 369us/step - loss: 0.5899 - val_loss: 0.6372\n",
      "Epoch 197/205\n",
      "1660/1660 [==============================] - 1s 365us/step - loss: 0.5889 - val_loss: 0.6467\n",
      "Epoch 198/205\n",
      "1660/1660 [==============================] - 1s 316us/step - loss: 0.5899 - val_loss: 0.6354\n",
      "Epoch 199/205\n",
      "1660/1660 [==============================] - 1s 310us/step - loss: 0.5884 - val_loss: 0.6495\n",
      "Epoch 200/205\n",
      "1660/1660 [==============================] - 1s 320us/step - loss: 0.5899 - val_loss: 0.6342\n",
      "Epoch 201/205\n",
      "1660/1660 [==============================] - 1s 347us/step - loss: 0.5878 - val_loss: 0.6756\n",
      "Epoch 202/205\n",
      "1660/1660 [==============================] - 1s 319us/step - loss: 0.5893 - val_loss: 0.6341\n",
      "Epoch 203/205\n",
      "1660/1660 [==============================] - 1s 319us/step - loss: 0.5874 - val_loss: 0.6752\n",
      "Epoch 204/205\n",
      "1660/1660 [==============================] - 0s 296us/step - loss: 0.5890 - val_loss: 0.6338\n",
      "Epoch 205/205\n",
      "1660/1660 [==============================] - 1s 307us/step - loss: 0.5871 - val_loss: 0.6758\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "history = model.fit(X_train, y_train, epochs=205, batch_size=12,\n",
    "                    validation_data=(X_test, y_test), verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:20:05.481685Z",
     "start_time": "2018-11-06T01:20:05.328477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:20:05.678536Z",
     "start_time": "2018-11-06T01:20:05.485466Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(X_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:20:05.701626Z",
     "start_time": "2018-11-06T01:20:05.680787Z"
    }
   },
   "outputs": [],
   "source": [
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, X_test[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:20:05.725985Z",
     "start_time": "2018-11-06T01:20:05.703652Z"
    }
   },
   "outputs": [],
   "source": [
    "# invert scaling for actual\n",
    "y_test = y_test.reshape((len(y_test), 1))\n",
    "inv_y = concatenate((y_test, X_test[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:20:05.750146Z",
     "start_time": "2018-11-06T01:20:05.727725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.466\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T01:24:21.847304Z",
     "start_time": "2018-11-06T01:24:21.803465Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4e9405d7d7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
